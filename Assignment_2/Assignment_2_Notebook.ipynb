{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Convolutional Neural Networks and AutoML\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries, including PyTorch and the chosen hyperparameter optimization library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "from support import load_dataset\n",
    "\n",
    "# Importing hyperparameter optimization library\n",
    "import optuna\n",
    "# from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess the Wefabricate Dataset\n",
    "Load the Wefabricate dataset and preprocess it for use with a CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_dataset()\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the CNN Model\n",
    "Define a CNN to classify product images in the Wefabricate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN Model\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)  # Input channels = 3, output channels = 6, kernel size = 5\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Max pooling over a (2, 2) window\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)  # Input channels = 6, output channels = 16, kernel size = 5\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)  # 10 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass data through conv1\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # Pass data through conv2\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Flatten data\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        # Pass data through fc1\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Pass data through fc2\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # Pass data through fc3\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the network\n",
    "model = ConvNet()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the CNN Model\n",
    "Train the CNN model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:23,  4.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.3364688396453857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:21,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 2.3165061473846436\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Move the model to the device\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# Training the CNN Model\n",
    "for epoch in range(2):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(train_loader, 0)):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Print the loss every epoch\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the CNN Model\n",
    "Test the trained CNN model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the CNN Model\n",
    "correct = 0\n",
    "total = 0\n",
    "# Since we're testing, we don't need to calculate gradients\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        # Calculate outputs by running images through the network\n",
    "        outputs = model(images)\n",
    "        # The class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Selection\n",
    "Choose five hyperparameters to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters to tune\n",
    "hyperparameters = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001, 0.0001],  # Learning rate for the optimizer\n",
    "    'batch_size': [16, 32, 64, 128],  # Batch size for the data loaders\n",
    "    'num_epochs': [5, 10, 15, 20],  # Number of epochs for training\n",
    "    'optimizer': ['SGD', 'Adam'],  # Type of optimizer\n",
    "    'momentum': [0.9, 0.95, 0.99]  # Momentum for SGD optimizer\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization with Random Search\n",
    "Use random search to tune the chosen hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Optimization with Random Search\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "# Define the number of iterations for the random search\n",
    "n_iter_search = 20\n",
    "\n",
    "# Create a random grid\n",
    "random_grid = ParameterSampler(hyperparameters, n_iter=n_iter_search)\n",
    "\n",
    "# Function to train and evaluate a model\n",
    "def train_and_evaluate_model(params):\n",
    "    # Create a new model\n",
    "    model = ConvNet()\n",
    "    \n",
    "    # Choose the optimizer\n",
    "    if params['optimizer'] == 'SGD':\n",
    "        optimizer = SGD(model.parameters(), lr=params['learning_rate'], momentum=params['momentum'])\n",
    "    else:\n",
    "        optimizer = Adam(model.parameters(), lr=params['learning_rate'])\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_data, batch_size=params['batch_size'], shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=params['batch_size'], shuffle=False)\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(params['num_epochs']):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    # Test the model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Perform the random search\n",
    "best_params = None\n",
    "best_accuracy = 0\n",
    "for params in random_grid:\n",
    "    accuracy = train_and_evaluate_model(params)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_params = params\n",
    "\n",
    "print('Best parameters found by random search:', best_params)\n",
    "print('Best accuracy found by random search:', best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization with Sophisticated Method\n",
    "Use a more sophisticated method than random search to tune the chosen hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Optimization with Sophisticated Method\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters for this trial\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64, 128]),\n",
    "        'num_epochs': trial.suggest_categorical('num_epochs', [5, 10, 15, 20]),\n",
    "        'optimizer': trial.suggest_categorical('optimizer', ['SGD', 'Adam']),\n",
    "        'momentum': trial.suggest_uniform('momentum', 0.9, 0.99)\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate a model with these hyperparameters\n",
    "    accuracy = train_and_evaluate_model(params)\n",
    "    \n",
    "    # The objective is to maximize accuracy\n",
    "    return accuracy\n",
    "\n",
    "# Create a study to optimize the hyperparameters\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Run the hyperparameter optimization\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print the result\n",
    "best_params = study.best_params\n",
    "best_accuracy = study.best_value\n",
    "\n",
    "print('Best parameters found by sophisticated search:', best_params)\n",
    "print('Best accuracy found by sophisticated search:', best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-Fold Cross-Validation\n",
    "Perform 5-fold cross-validation on the training set to calculate the validation accuracy for the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "num_folds = 5\n",
    "\n",
    "# Define the KFold object\n",
    "kf = KFold(n_splits=num_folds)\n",
    "\n",
    "# Convert the train data to numpy arrays for easier indexing\n",
    "train_images = np.array(train_data.imgs)\n",
    "train_labels = np.array(train_data.targets)\n",
    "\n",
    "# Initialize a list to store the validation accuracies for each fold\n",
    "validation_accuracies = []\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "for train_index, val_index in kf.split(train_images):\n",
    "    # Split the data into the training set and the validation set\n",
    "    train_images_fold, val_images_fold = train_images[train_index], train_images[val_index]\n",
    "    train_labels_fold, val_labels_fold = train_labels[train_index], train_labels[val_index]\n",
    "    \n",
    "    # Convert the numpy arrays back to datasets\n",
    "    train_data_fold = torch.utils.data.TensorDataset(torch.from_numpy(train_images_fold), torch.from_numpy(train_labels_fold))\n",
    "    val_data_fold = torch.utils.data.TensorDataset(torch.from_numpy(val_images_fold), torch.from_numpy(val_labels_fold))\n",
    "    \n",
    "    # Create data loaders for this fold\n",
    "    train_loader_fold = DataLoader(train_data_fold, batch_size=64, shuffle=True)\n",
    "    val_loader_fold = DataLoader(val_data_fold, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # Train the model on this fold\n",
    "    for epoch in range(10):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader_fold, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    # Test the model on the validation set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader_fold:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Calculate the accuracy for this fold and add it to the list of validation accuracies\n",
    "    fold_accuracy = correct / total\n",
    "    validation_accuracies.append(fold_accuracy)\n",
    "\n",
    "# Calculate the average validation accuracy over all folds\n",
    "average_validation_accuracy = sum(validation_accuracies) / num_folds\n",
    "\n",
    "print('Average validation accuracy from 5-fold cross-validation:', average_validation_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Results Before and After Hyperparameter Optimization\n",
    "Compare the results obtained before and after automatic hyperparameter optimization in terms of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Results Before and After Hyperparameter Optimization\n",
    "\n",
    "# Store the accuracy before hyperparameter optimization\n",
    "accuracy_before_optimization = 100 * correct / total\n",
    "\n",
    "# Store the accuracy after hyperparameter optimization\n",
    "accuracy_after_optimization = best_accuracy\n",
    "\n",
    "# Print the results\n",
    "print(f'Accuracy before hyperparameter optimization: {accuracy_before_optimization}%')\n",
    "print(f'Accuracy after hyperparameter optimization: {accuracy_after_optimization}%')\n",
    "\n",
    "# Compare the results\n",
    "if accuracy_after_optimization > accuracy_before_optimization:\n",
    "    print(\"The accuracy improved after hyperparameter optimization.\")\n",
    "elif accuracy_after_optimization < accuracy_before_optimization:\n",
    "    print(\"The accuracy decreased after hyperparameter optimization.\")\n",
    "else:\n",
    "    print(\"The accuracy did not change after hyperparameter optimization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model Weights Before and After Hyperparameter Tuning\n",
    "Save the trained model weights before and after hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model weights before hyperparameter tuning\n",
    "torch.save(model.state_dict(), 'model_weights_before_tuning.pth')\n",
    "\n",
    "# Load the model with the best parameters found by sophisticated search\n",
    "best_model = ConvNet()\n",
    "if best_params['optimizer'] == 'SGD':\n",
    "    best_optimizer = SGD(best_model.parameters(), lr=best_params['learning_rate'], momentum=best_params['momentum'])\n",
    "else:\n",
    "    best_optimizer = Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "# Train the best model\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        best_optimizer.zero_grad()\n",
    "        outputs = best_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        best_optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "# Save the model weights after hyperparameter tuning\n",
    "torch.save(best_model.state_dict(), 'model_weights_after_tuning.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1BM120",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
