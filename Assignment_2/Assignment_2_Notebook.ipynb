{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Convolutional Neural Networks and AutoML\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries, including PyTorch and the chosen hyperparameter optimization library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "from support import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing hyperparameter optimization library\n",
    "import optuna\n",
    "# from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess the Wefabricate Dataset\n",
    "Load the Wefabricate dataset and preprocess it for use with a CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_dataset()\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data with their labels\n",
    "\n",
    "# Define the class labels \n",
    "class_labels = [\"Defect\", \"No Defect\"]\n",
    "\n",
    "# Get a batch of training data\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# Create a grid of the images and print the labels\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i in range(5):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow(images[i][0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(class_labels[labels[i]])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the CNN Model\n",
    "Define a CNN to classify product images in the Wefabricate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN Model\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, padding=0, stride=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.pool2 = nn.MaxPool2d(3, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 8 * 3, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 8 * 3)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the network\n",
    "model = ConvNet()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the CNN Model\n",
    "Train the CNN model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the device\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# Training the CNN Model\n",
    "for epoch in range(1):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(train_loader, 0)):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        labels = labels.float()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print the loss every epoch\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the CNN Model\n",
    "Test the trained CNN model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        predicted = torch.round(\n",
    "            torch.sigmoid(outputs)\n",
    "        ).squeeze()  # Ensure predicted is 1D\n",
    "        labels = labels.float()  # Ensure labels are float for comparison\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Selection\n",
    "Choose five hyperparameters to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization with Random Search\n",
    "Use random search to tune the chosen hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization with Sophisticated Method\n",
    "Use a more sophisticated method than random search to tune the chosen hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-Fold Cross-Validation\n",
    "Perform 5-fold cross-validation on the training set to calculate the validation accuracy for the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Results Before and After Hyperparameter Optimization\n",
    "Compare the results obtained before and after automatic hyperparameter optimization in terms of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model Weights Before and After Hyperparameter Tuning\n",
    "Save the trained model weights before and after hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model weights before hyperparameter tuning\n",
    "torch.save(model.state_dict(), 'model_weights_before_tuning.pth')\n",
    "\n",
    "# Load the model with the best parameters found by sophisticated search\n",
    "best_model = ConvNet()\n",
    "if best_params['optimizer'] == 'SGD':\n",
    "    best_optimizer = SGD(best_model.parameters(), lr=best_params['learning_rate'], momentum=best_params['momentum'])\n",
    "else:\n",
    "    best_optimizer = Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "# Train the best model\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        best_optimizer.zero_grad()\n",
    "        outputs = best_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        best_optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "# Save the model weights after hyperparameter tuning\n",
    "torch.save(best_model.state_dict(), 'model_weights_after_tuning.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1BM120",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
