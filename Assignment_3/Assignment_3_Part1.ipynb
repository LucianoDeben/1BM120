{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Libraries\n",
    "Install the necessary libraries, including Gymnasium, Stable Baselines3, and SB3 Contrib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required modules\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "\n",
    "# Import BoundedKnapsack Environment\n",
    "from knapsack_env import BoundedKnapsackEnv\n",
    "\n",
    "# Setting the seed for reproducibility\n",
    "seed = 2024\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global variables\n",
    "TIME_STEPS = 1000\n",
    "EVAL_EPISODES = 100\n",
    "VERBOSE = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an Instance of the Environment\n",
    "Create an instance of the BoundedKnapsack environment with the specified parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Space: (array([[ 82,  65,  60,  69,  35,  87,  35,  48,  40,  23,  15,  86,   3,\n",
      "         41,  26,  64,  42,  52,  95,  67,  17,  45,  93,  30,  53,  50,\n",
      "         20,  74,  49,  66,   5,  48,  51,  19,  71,  51,  10,  93,  44,\n",
      "         55,  28,  93,  31,  37,  88,  30,   6,   3,   9,   9,  37,  15,\n",
      "         38,  83,  39,  26,  33,  98,  69,  82,  25,  90,  18,  57,  95,\n",
      "         95,  60,  65,  38,  86,  57,  80,  75,  70,  10,  19,  20,  66,\n",
      "         57,  42,  36,  30,  63,  35,  22,  34,  59,  80,  48,  56,  70,\n",
      "         55,  20,  21,  83,  57,  43,  66,  71,  79,   3,  99,   7,  54,\n",
      "         58,  79,  69,  43,  40,   6,  34,  73,  15,  19,  32,  82,  31,\n",
      "          9,   2,  31,  54,  83,  68,  44,  59,  41,  63,  28,  95,  48,\n",
      "         36,  98,  91,  71,  39,  31,  60,  11,  33,  10,  94,  93,  30,\n",
      "         36,  34,  20,  99,  80,  85,  89,  49,  79,  85,  76,   3,  50,\n",
      "         63,  11,  83,  40,  23,  68,  29,  55,  43,  12,  28,  21,  52,\n",
      "         52,  44,  45,  46,  93,  24,  26,  26,  17,  59,  62,  98,  71,\n",
      "         30,  21,  76,  46,  80,   9,  69,  93,  95,  15,  68,  36,  86,\n",
      "         43,  66,  64,  66,  10, 200],\n",
      "       [ 89,  97,  29,  59,  83,  95,   2,   6,  33,  55,  75,  36,   5,\n",
      "          9,  75,  25,  40,  94,  71,  15,  21,  77,   7,  61,  81,  93,\n",
      "         85,  30,  47,  14,  85,  18,  27,  92,   8,  54,  63,  37,  67,\n",
      "         98,  37,  86,  30,   3,  50,  55,  80,  36,  74,  53,   3,  31,\n",
      "         28,  40,  64,  44,  41,  61,  96,  38,  75,  15,  21,  59,  16,\n",
      "         94,  94,  34,  86,  30,  79,  78,  45,  97,  88,  70,  19,  36,\n",
      "          5,  24,  42,  52,  64,  63,  85,  11,  72,  73,  10,  82,  15,\n",
      "          0,  90,  73,  21,   4,  24,  96,  23,  86,  38,   6,  69,  43,\n",
      "         44,  43,  49,  57,  98,  79,  54,  78,  57,   2,  83,  93,  76,\n",
      "         27,  85,  26,  92,  52,  28,  58,  76,  31,  66,  66,  83,  83,\n",
      "         54,  44,  72,  72,  76,  48,  42,  53,  84,  58,  23,  45,  17,\n",
      "         37,   5,  58,  19,  92,  86,  90,  23,  73,   9,  13,   0,  79,\n",
      "         61,  62,  91,  24,  14,  97,  81,  94,  73,  54,  54,   4,  86,\n",
      "         80,  71,  60,  90,  62,  51,  96,  49,  63,  87,  90,  90,  17,\n",
      "          8,  28,  69,   9,  72,  31,  72,  34,   6,  52,  49,  50,  42,\n",
      "         35,  82,  40,   5,   8,   0],\n",
      "       [  5,   8,   3,   5,   3,   6,   6,   1,   9,   9,   2,   9,   8,\n",
      "          2,   5,   9,   1,   3,   3,   4,   4,   1,   4,   2,   7,   7,\n",
      "          5,   2,   9,   3,   4,   2,   4,   6,   2,   8,   5,   3,   3,\n",
      "          1,   3,   4,   1,   9,   4,   3,   3,   5,   5,   4,   3,   9,\n",
      "          9,   5,   3,   7,   7,   7,   2,   3,   9,   1,   3,   5,   8,\n",
      "          5,   1,   8,   6,   8,   5,   1,   4,   4,   1,   5,   5,   5,\n",
      "          3,   1,   7,   9,   8,   4,   7,   5,   6,   3,   2,   1,   3,\n",
      "          3,   8,   2,   9,   6,   2,   6,   1,   9,   4,   5,   1,   4,\n",
      "          4,   1,   2,   8,   1,   5,   9,   3,   5,   2,   1,   2,   2,\n",
      "          9,   3,   6,   1,   1,   3,   8,   1,   8,   5,   3,   5,   7,\n",
      "          1,   6,   3,   2,   3,   8,   5,   9,   7,   1,   6,   6,   6,\n",
      "          3,   3,   4,   4,   8,   2,   3,   4,   6,   3,   3,   8,   1,\n",
      "          5,   3,   6,   6,   6,   5,   2,   4,   5,   9,   5,   8,   2,\n",
      "          6,   6,   8,   8,   7,   9,   6,   4,   4,   5,   4,   2,   7,\n",
      "          9,   8,   3,   8,   8,   6,   3,   4,   9,   4,   9,   1,   6,\n",
      "          3,   1,   8,   9,   7,   0]]), {})\n",
      "Action Space Size: 200\n"
     ]
    }
   ],
   "source": [
    "# Enable the environment\n",
    "env = BoundedKnapsackEnv(n_items=200, max_weight=200)\n",
    "\n",
    "# Inspect the state space and action spaces\n",
    "state_space = env.reset()\n",
    "action_space_size = env.action_space.n\n",
    "\n",
    "# Print the state space and action space size\n",
    "print(f\"State Space: {state_space}\")\n",
    "print(f\"Action Space Size: {action_space_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test DRL Agents\n",
    "Train and test at least two different DRL agents using the algorithms provided in Stable Baselines3 with default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=186.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=186.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1500, episode_reward=186.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2000, episode_reward=186.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.25 GiB for an array with shape (1000000, 1, 3, 201) and data type int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m ppo_model_small\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39mTIME_STEPS, callback\u001b[38;5;241m=\u001b[39meval_callback_ppo)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Training the DQN agent\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m dqn_model_small \u001b[38;5;241m=\u001b[39m \u001b[43mDQN\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMlpPolicy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_dqn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVERBOSE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m dqn_model_small\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39mTIME_STEPS, callback\u001b[38;5;241m=\u001b[39meval_callback_dqn)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Evaluating the PPO agentP\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\20191678\\AppData\\Local\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:141\u001b[0m, in \u001b[0;36mDQN.__init__\u001b[1;34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, target_update_interval, exploration_fraction, exploration_initial_eps, exploration_final_eps, max_grad_norm, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploration_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _init_setup_model:\n\u001b[1;32m--> 141\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\20191678\\AppData\\Local\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:144\u001b[0m, in \u001b[0;36mDQN._setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_setup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_aliases()\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;66;03m# Copy running stats, see GH issue #996\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\20191678\\AppData\\Local\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:189\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must pass an environment when using `HerReplayBuffer`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    188\u001b[0m         replay_buffer_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_envs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimize_memory_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize_memory_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mreplay_buffer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_class(\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space,\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space,\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_schedule,\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_kwargs,\n\u001b[0;32m    204\u001b[0m )\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\20191678\\AppData\\Local\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\stable_baselines3\\common\\buffers.py:216\u001b[0m, in \u001b[0;36mReplayBuffer.__init__\u001b[1;34m(self, buffer_size, observation_space, action_space, device, n_envs, optimize_memory_usage, handle_timeout_termination)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_envs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_shape), dtype\u001b[38;5;241m=\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m optimize_memory_usage:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When optimizing memory, `observations` contains also the next observation\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_observations \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_envs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[0;32m    219\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_envs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dim), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_dtype(action_space\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    220\u001b[0m )\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_envs), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.25 GiB for an array with shape (1000000, 1, 3, 201) and data type int32"
     ]
    }
   ],
   "source": [
    "# Define the policy network architecture\n",
    "policy_kwargs = dict(activation_fn=torch.nn.Tanh, net_arch=[64, 64])\n",
    "\n",
    "# Create a log directory\n",
    "log_dir = './logs/'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Wrap the environment with Monitor\n",
    "env_ppo = Monitor(env, log_dir + 'ppo/')\n",
    "env_dqn = Monitor(env, log_dir + 'dqn/')\n",
    "\n",
    "# Setup evaluation callback for logging\n",
    "eval_callback_ppo = EvalCallback(env_ppo, best_model_save_path='./models/ppo/',\n",
    "                                 log_path=log_dir + 'ppo/', eval_freq=500,\n",
    "                                 deterministic=True, render=False)\n",
    "\n",
    "eval_callback_dqn = EvalCallback(env_dqn, best_model_save_path='./models/dqn/',\n",
    "                                 log_path=log_dir + 'dqn/', eval_freq=500,\n",
    "                                 deterministic=True, render=False)\n",
    "\n",
    "# Training the PPO agent\n",
    "ppo_model_small = PPO(\"MlpPolicy\", env_ppo, policy_kwargs=policy_kwargs, verbose=VERBOSE)\n",
    "ppo_model_small.learn(total_timesteps=TIME_STEPS, callback=eval_callback_ppo)\n",
    "\n",
    "# Training the DQN agent\n",
    "dqn_model_small = DQN(\"MlpPolicy\", env_dqn, policy_kwargs=policy_kwargs, verbose=VERBOSE)\n",
    "dqn_model_small.learn(total_timesteps=TIME_STEPS, callback=eval_callback_dqn)\n",
    "\n",
    "# Evaluating the PPO agentP\n",
    "mean_reward, std_reward = evaluate_policy(ppo_model_small, env_ppo, n_eval_episodes=EVAL_EPISODES)\n",
    "\n",
    "print(f\"Mean reward for PPO agent: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "# Evaluating the DQN agent\n",
    "mean_reward, std_reward = evaluate_policy(dqn_model_small, env_dqn, n_eval_episodes=EVAL_EPISODES)\n",
    "\n",
    "print(f\"Mean reward for DQN agent: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "def moving_average(values, window):\n",
    "    \"\"\"\n",
    "    Compute the moving average of a numpy array.\n",
    "    \"\"\"\n",
    "    weights = np.repeat(1.0, window) / window\n",
    "    return np.convolve(values, weights, 'valid')\n",
    "\n",
    "def plot_results(log_folder, title='Training Reward', window=50):\n",
    "    \"\"\"\n",
    "    Plots the training reward from the log folder with a moving average smoothing.\n",
    "    \"\"\"\n",
    "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
    "    y_smooth = moving_average(y, window=window)\n",
    "    # Adjust x to match the length of the smoothed array\n",
    "    x_smooth = x[:len(y_smooth)]\n",
    "    \n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.plot(x_smooth, y_smooth)\n",
    "    plt.xlabel('Number of Timesteps')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_results('./logs/ppo/', 'PPO Training Reward', window=50)\n",
    "plot_results('./logs/dqn/', 'DQN Training Reward', window=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with Different Neural Network Architectures\n",
    "Experiment with different neural network architectures for the DRL agents with default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the policy network architecture\n",
    "policy_kwargs = dict(activation_fn=torch.nn.ReLU, net_arch=[128, 128])\n",
    "\n",
    "# Training the PPO agent\n",
    "ppo_model_large = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=VERBOSE)\n",
    "ppo_model_large.learn(total_timesteps=TIME_STEPS)\n",
    "\n",
    "# Training the DQN agent\n",
    "dqn_model_large = DQN(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=VERBOSE)\n",
    "dqn_model_large.learn(total_timesteps=TIME_STEPS)\n",
    "\n",
    "# Evaluating the PPO agentP\n",
    "mean_reward, std_reward = evaluate_policy(ppo_model_large, env, n_eval_episodes=EVAL_EPISODES)\n",
    "\n",
    "print(f\"Mean reward for PPO agent: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "# Evaluating the DQN agent\n",
    "mean_reward, std_reward = evaluate_policy(dqn_model_large, env, n_eval_episodes=EVAL_EPISODES)\n",
    "\n",
    "print(f\"Mean reward for DQN agent: {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune the Algorithms Hyperparameters\n",
    "Tune the hyperparameters of the algorithms by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the policy network architecture\n",
    "policy_kwargs = dict(activation_fn=torch.nn.ReLU, net_arch=[128, 128])\n",
    "\n",
    "# Tuning the hyperparameters of the PPO agent\n",
    "ppo_model_tuned = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=VERBOSE, learning_rate=0.0003, n_steps=2048, batch_size=64, n_epochs=10, gamma=0.99, gae_lambda=0.95, clip_range=0.2)\n",
    "ppo_model_tuned.learn(total_timesteps=TIME_STEPS)\n",
    "\n",
    "# Evaluating the PPO agent\n",
    "mean_reward_ppo, std_reward_ppo = evaluate_policy(ppo_model_tuned, env, n_eval_episodes=EVAL_EPISODES)\n",
    "print(f\"Mean reward for PPO agent after hyperparameter tuning: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "# Tuning the hyperparameters of the DQN agent\n",
    "dqn_model_tuned = DQN(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=VERBOSE, learning_rate=0.0005, buffer_size=10000, learning_starts=1000, batch_size=64, tau=1.0, gamma=0.99, train_freq=4, gradient_steps=1)\n",
    "dqn_model_tuned.learn(total_timesteps=TIME_STEPS)\n",
    "\n",
    "# Evaluating the DQN agent\n",
    "mean_reward_dqn, std_reward_dqn = evaluate_policy(dqn_model_tuned, env, n_eval_episodes=EVAL_EPISODES)\n",
    "print(f\"Mean reward for DQN agent after hyperparameter tuning: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "# Tuning the hyperparameters of the A2C agent\n",
    "a2c_model_tuned = A2C(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=VERBOSE, learning_rate=0.0007, n_steps=5, gamma=0.99, gae_lambda=1.0, ent_coef=0.0, vf_coef=0.5, max_grad_norm=0.5, use_rms_prop=False, use_sde=False)\n",
    "a2c_model_tuned.learn(total_timesteps=TIME_STEPS)\n",
    "\n",
    "# Evaluating the A2C agent\n",
    "mean_reward_a2c, std_reward_a2c = evaluate_policy(a2c_model_tuned, env, n_eval_episodes=EVAL_EPISODES)\n",
    "print(f\"Mean reward for A2C agent after hyperparameter tuning: {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Agents and Compare Results\n",
    "Evaluate the performance of the agents and compare the best results obtained using the different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the agents and their corresponding mean rewards and standard deviations\n",
    "agents = ['PPO', 'DQN', 'A2C']\n",
    "mean_rewards = [mean_reward_ppo, mean_reward_dqn, mean_reward_a2c]\n",
    "std_rewards = [std_reward_ppo, std_reward_dqn, std_reward_a2c]\n",
    "\n",
    "# Plotting the mean rewards\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(agents, mean_rewards, yerr=std_rewards, align='center', alpha=0.7, ecolor='black', capsize=10)\n",
    "plt.ylabel('Mean Reward')\n",
    "plt.title('Comparison of Mean Rewards of Different Agents')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the agents and their corresponding mean rewards, standard deviations, and models\n",
    "agents = ['PPO', 'DQN', 'A2C']\n",
    "mean_rewards = [mean_reward_ppo, mean_reward_dqn, mean_reward_a2c]\n",
    "models = [ppo_model_tuned, dqn_model_tuned, a2c_model_tuned]\n",
    "\n",
    "# Determine the index of the best model\n",
    "best_index = np.argmax(mean_rewards)\n",
    "\n",
    "# Save the best model\n",
    "models[best_index].save(\"models/best_model_part1\")\n",
    "\n",
    "print(f\"The best model is {agents[best_index]} with a mean reward of {mean_rewards[best_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2\n",
    "\n",
    "---\n",
    "\n",
    "## Enabling action masking, train and test a MaskablePPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for the mask\n",
    "def mask_fn(env):\n",
    "    return env.get_mask()\n",
    "\n",
    "# Create an instance of the environment with mask enabled\n",
    "env = BoundedKnapsackEnv(n_items=200, max_weight=200, mask=True)\n",
    "\n",
    "# Wrap the environment with the ActionMasker\n",
    "vec_env = ActionMasker(env, mask_fn)\n",
    "\n",
    "# Define the policy architecture\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi=[128, 128, 128], vf=[128, 128, 128])],  \n",
    ")\n",
    "\n",
    "# Train a MaskablePPO agent\n",
    "model = MaskablePPO(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=VERBOSE,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    learning_rate=3e-4,\n",
    "    ent_coef=0.0,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    target_kl=None,\n",
    ")\n",
    "\n",
    "# Adjust timesteps for meaningful training\n",
    "model.learn(total_timesteps=TIME_STEPS, use_masking=True)\n",
    "\n",
    "# Evaluate the agent\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=EVAL_EPISODES)\n",
    "\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with different neural network architectures and tune the algorithm hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of configurations to experiment with\n",
    "configurations = [\n",
    "    {\n",
    "        \"policy_kwargs\": dict(net_arch=[dict(pi=[64, 64], vf=[64, 64])]),\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"n_steps\": 2048,\n",
    "        \"batch_size\": 64,\n",
    "    },\n",
    "    {\n",
    "        \"policy_kwargs\": dict(net_arch=[dict(pi=[256, 256], vf=[256, 256])]),\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"n_steps\": 1024,\n",
    "        \"batch_size\": 128,\n",
    "    },\n",
    "    {\n",
    "        \"policy_kwargs\": dict(net_arch=[dict(pi=[128, 128], vf=[128, 128])]),\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"n_steps\": 4096,\n",
    "        \"batch_size\": 32,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Initialize best mean reward to negative infinity\n",
    "best_mean_reward = -np.inf\n",
    "\n",
    "# Loop over the architectures and learning rates\n",
    "for config in configurations:\n",
    "    policy_kwargs = config[\"policy_kwargs\"]\n",
    "    model = MaskablePPO(\n",
    "        \"MlpPolicy\",\n",
    "        vec_env,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        verbose=VERBOSE,\n",
    "        n_steps=config[\"n_steps\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        ent_coef=0.0,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        vf_coef=0.5,\n",
    "        max_grad_norm=0.5,\n",
    "        target_kl=None,\n",
    "    )\n",
    "    model.learn(total_timesteps=TIME_STEPS, use_masking=True)\n",
    "    mean_reward_mppo, std_reward_mppo = evaluate_policy(model, env, n_eval_episodes=EVAL_EPISODES)\n",
    "    print(f\"Architecture: {config['policy_kwargs']['net_arch']}, Learning Rate: {config['learning_rate']}, Mean reward: {mean_reward} +/- {std_reward}\")    \n",
    "    # If this mean reward is greater than the current best, save this model\n",
    "    if mean_reward_mppo > best_mean_reward:\n",
    "        best_mean_reward = mean_reward_mppo\n",
    "        model.save(\"models/best_model_part2\")\n",
    "        print(f\"New best mean reward: {mean_reward_mppo} +/- {std_reward_mppo}, model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the agent and compare the best results obtained with those of the best agent from Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model from Part 1\n",
    "if agents[best_index] == 'PPO':\n",
    "    best_model_part1 = PPO.load(\"best_model_part1\", env=env)\n",
    "elif agents[best_index] == 'DQN':\n",
    "    best_model_part1 = DQN.load(\"best_model_part1\", env=env)\n",
    "elif agents[best_index] == 'A2C':\n",
    "    best_model_part1 = A2C.load(\"best_model_part1\", env=env)\n",
    "else:\n",
    "    print(\"Unknown model type\")\n",
    "    \n",
    "# Load the best model from Part 2\n",
    "best_model_part2 = MaskablePPO.load(\"best_model_part2\", env=env)\n",
    "\n",
    "# Evaluate the best model from Part 1\n",
    "mean_reward_part1, std_reward_part1 = evaluate_policy(best_model_part1, env, n_eval_episodes=EVAL_EPISODES)\n",
    "\n",
    "# Evaluate the best model from Part 2\n",
    "mean_reward_part2, std_reward_part2 = evaluate_policy(best_model_part2, env, n_eval_episodes=EVAL_EPISODES)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Part 1 - Mean reward: {mean_reward_part1} +/- {std_reward_part1}\")\n",
    "print(f\"Part 2 - Mean reward: {mean_reward_part2} +/- {std_reward_part2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
