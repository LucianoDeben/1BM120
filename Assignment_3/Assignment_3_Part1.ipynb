{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Libraries\n",
    "Install the necessary libraries, including Gymnasium, Stable Baselines3, and SB3 Contrib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gymnasium\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "\n",
    "# Import BoundedKnapsack Environment\n",
    "from knapsack_env import BoundedKnapsackEnv\n",
    "\n",
    "# Setting the seed for reproducibility\n",
    "seed = 2024\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global variables\n",
    "TIME_STEPS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an Instance of the Environment\n",
    "Create an instance of the BoundedKnapsack environment with the specified parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Space: (array([[ 82,  65,  60,  69,  35,  87,  35,  48,  40,  23,  15,  86,   3,\n",
      "         41,  26,  64,  42,  52,  95,  67,  17,  45,  93,  30,  53,  50,\n",
      "         20,  74,  49,  66,   5,  48,  51,  19,  71,  51,  10,  93,  44,\n",
      "         55,  28,  93,  31,  37,  88,  30,   6,   3,   9,   9,  37,  15,\n",
      "         38,  83,  39,  26,  33,  98,  69,  82,  25,  90,  18,  57,  95,\n",
      "         95,  60,  65,  38,  86,  57,  80,  75,  70,  10,  19,  20,  66,\n",
      "         57,  42,  36,  30,  63,  35,  22,  34,  59,  80,  48,  56,  70,\n",
      "         55,  20,  21,  83,  57,  43,  66,  71,  79,   3,  99,   7,  54,\n",
      "         58,  79,  69,  43,  40,   6,  34,  73,  15,  19,  32,  82,  31,\n",
      "          9,   2,  31,  54,  83,  68,  44,  59,  41,  63,  28,  95,  48,\n",
      "         36,  98,  91,  71,  39,  31,  60,  11,  33,  10,  94,  93,  30,\n",
      "         36,  34,  20,  99,  80,  85,  89,  49,  79,  85,  76,   3,  50,\n",
      "         63,  11,  83,  40,  23,  68,  29,  55,  43,  12,  28,  21,  52,\n",
      "         52,  44,  45,  46,  93,  24,  26,  26,  17,  59,  62,  98,  71,\n",
      "         30,  21,  76,  46,  80,   9,  69,  93,  95,  15,  68,  36,  86,\n",
      "         43,  66,  64,  66,  10, 200],\n",
      "       [ 89,  97,  29,  59,  83,  95,   2,   6,  33,  55,  75,  36,   5,\n",
      "          9,  75,  25,  40,  94,  71,  15,  21,  77,   7,  61,  81,  93,\n",
      "         85,  30,  47,  14,  85,  18,  27,  92,   8,  54,  63,  37,  67,\n",
      "         98,  37,  86,  30,   3,  50,  55,  80,  36,  74,  53,   3,  31,\n",
      "         28,  40,  64,  44,  41,  61,  96,  38,  75,  15,  21,  59,  16,\n",
      "         94,  94,  34,  86,  30,  79,  78,  45,  97,  88,  70,  19,  36,\n",
      "          5,  24,  42,  52,  64,  63,  85,  11,  72,  73,  10,  82,  15,\n",
      "          0,  90,  73,  21,   4,  24,  96,  23,  86,  38,   6,  69,  43,\n",
      "         44,  43,  49,  57,  98,  79,  54,  78,  57,   2,  83,  93,  76,\n",
      "         27,  85,  26,  92,  52,  28,  58,  76,  31,  66,  66,  83,  83,\n",
      "         54,  44,  72,  72,  76,  48,  42,  53,  84,  58,  23,  45,  17,\n",
      "         37,   5,  58,  19,  92,  86,  90,  23,  73,   9,  13,   0,  79,\n",
      "         61,  62,  91,  24,  14,  97,  81,  94,  73,  54,  54,   4,  86,\n",
      "         80,  71,  60,  90,  62,  51,  96,  49,  63,  87,  90,  90,  17,\n",
      "          8,  28,  69,   9,  72,  31,  72,  34,   6,  52,  49,  50,  42,\n",
      "         35,  82,  40,   5,   8,   0],\n",
      "       [  5,   8,   3,   5,   3,   6,   6,   1,   9,   9,   2,   9,   8,\n",
      "          2,   5,   9,   1,   3,   3,   4,   4,   1,   4,   2,   7,   7,\n",
      "          5,   2,   9,   3,   4,   2,   4,   6,   2,   8,   5,   3,   3,\n",
      "          1,   3,   4,   1,   9,   4,   3,   3,   5,   5,   4,   3,   9,\n",
      "          9,   5,   3,   7,   7,   7,   2,   3,   9,   1,   3,   5,   8,\n",
      "          5,   1,   8,   6,   8,   5,   1,   4,   4,   1,   5,   5,   5,\n",
      "          3,   1,   7,   9,   8,   4,   7,   5,   6,   3,   2,   1,   3,\n",
      "          3,   8,   2,   9,   6,   2,   6,   1,   9,   4,   5,   1,   4,\n",
      "          4,   1,   2,   8,   1,   5,   9,   3,   5,   2,   1,   2,   2,\n",
      "          9,   3,   6,   1,   1,   3,   8,   1,   8,   5,   3,   5,   7,\n",
      "          1,   6,   3,   2,   3,   8,   5,   9,   7,   1,   6,   6,   6,\n",
      "          3,   3,   4,   4,   8,   2,   3,   4,   6,   3,   3,   8,   1,\n",
      "          5,   3,   6,   6,   6,   5,   2,   4,   5,   9,   5,   8,   2,\n",
      "          6,   6,   8,   8,   7,   9,   6,   4,   4,   5,   4,   2,   7,\n",
      "          9,   8,   3,   8,   8,   6,   3,   4,   9,   4,   9,   1,   6,\n",
      "          3,   1,   8,   9,   7,   0]]), {})\n",
      "Action Space Size: 200\n"
     ]
    }
   ],
   "source": [
    "# Enable the environment\n",
    "env = BoundedKnapsackEnv(n_items=200, max_weight=200)\n",
    "\n",
    "# Inspect the state space and action spaces\n",
    "state_space = env.reset()\n",
    "action_space_size = env.action_space.n\n",
    "\n",
    "# Print the state space and action space size\n",
    "print(f\"State Space: {state_space}\")\n",
    "print(f\"Action Space Size: {action_space_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test DRL Agents\n",
    "Train and test at least two different DRL agents using the algorithms provided in Stable Baselines3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.72     |\n",
      "|    ep_rew_mean     | 192      |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-------------------------------------------\n",
      "| rollout/            |                   |\n",
      "|    ep_len_mean      | 4.5               |\n",
      "|    ep_rew_mean      | 168               |\n",
      "|    exploration_rate | 0.829             |\n",
      "| time/               |                   |\n",
      "|    episodes         | 4                 |\n",
      "|    fps              | 81064793292668928 |\n",
      "|    time_elapsed     | 0                 |\n",
      "|    total_timesteps  | 18                |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.88     |\n",
      "|    ep_rew_mean      | 190      |\n",
      "|    exploration_rate | 0.629    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 2495     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 39       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.17     |\n",
      "|    ep_rew_mean      | 198      |\n",
      "|    exploration_rate | 0.411    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 2739     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 62       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.12     |\n",
      "|    ep_rew_mean      | 194      |\n",
      "|    exploration_rate | 0.221    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 3622     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 82       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.2      |\n",
      "|    ep_rew_mean      | 197      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 3286     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 104      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.92     |\n",
      "|    ep_rew_mean      | 170      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 1648     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 118      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 27.1     |\n",
      "|    n_updates        | 4        |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.71     |\n",
      "|    ep_rew_mean      | 151      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 1182     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 132      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 43.1     |\n",
      "|    n_updates        | 7        |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.5      |\n",
      "|    ep_rew_mean      | 136      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 980      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 144      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 30.6     |\n",
      "|    n_updates        | 10       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.36     |\n",
      "|    ep_rew_mean      | 124      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 835      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 157      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 29.8     |\n",
      "|    n_updates        | 14       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20191678\\AppData\\Local\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\stable_baselines3\\common\\buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 4.84GB > 4.22GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.22     |\n",
      "|    ep_rew_mean      | 114      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 812      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 169      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 24.3     |\n",
      "|    n_updates        | 17       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.14     |\n",
      "|    ep_rew_mean      | 107      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 763      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 182      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 27       |\n",
      "|    n_updates        | 20       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.04     |\n",
      "|    ep_rew_mean      | 101      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 713      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 194      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 28       |\n",
      "|    n_updates        | 23       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.98     |\n",
      "|    ep_rew_mean      | 98.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 719      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 207      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 21.8     |\n",
      "|    n_updates        | 26       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.91     |\n",
      "|    ep_rew_mean      | 93.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 653      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 219      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 22.4     |\n",
      "|    n_updates        | 29       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.88     |\n",
      "|    ep_rew_mean      | 91.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 636      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 233      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 19.8     |\n",
      "|    n_updates        | 33       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.83     |\n",
      "|    ep_rew_mean      | 89.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 616      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 245      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 15.7     |\n",
      "|    n_updates        | 36       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.78     |\n",
      "|    ep_rew_mean      | 85.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 566      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 257      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 27.5     |\n",
      "|    n_updates        | 39       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.74     |\n",
      "|    ep_rew_mean      | 83       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 537      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 269      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 23.5     |\n",
      "|    n_updates        | 42       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.7      |\n",
      "|    ep_rew_mean      | 80       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 528      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 281      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 17.7     |\n",
      "|    n_updates        | 45       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.66     |\n",
      "|    ep_rew_mean      | 77.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 520      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 293      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 16.6     |\n",
      "|    n_updates        | 48       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.63     |\n",
      "|    ep_rew_mean      | 74.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 518      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 305      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 18       |\n",
      "|    n_updates        | 51       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.6      |\n",
      "|    ep_rew_mean      | 72.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 508      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 317      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 11.7     |\n",
      "|    n_updates        | 54       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.58     |\n",
      "|    ep_rew_mean      | 70.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 517      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 329      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 17.6     |\n",
      "|    n_updates        | 57       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.55     |\n",
      "|    ep_rew_mean      | 68.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 511      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 341      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 26       |\n",
      "|    n_updates        | 60       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.54     |\n",
      "|    ep_rew_mean      | 67.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 502      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 354      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 16.1     |\n",
      "|    n_updates        | 63       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.49     |\n",
      "|    ep_rew_mean      | 61.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 487      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 367      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 17.3     |\n",
      "|    n_updates        | 66       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.4      |\n",
      "|    ep_rew_mean      | 54.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 493      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 379      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 18.4     |\n",
      "|    n_updates        | 69       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.29     |\n",
      "|    ep_rew_mean      | 47.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 489      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 391      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 24.2     |\n",
      "|    n_updates        | 72       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.22     |\n",
      "|    ep_rew_mean      | 42.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 486      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 404      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 24       |\n",
      "|    n_updates        | 75       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.13     |\n",
      "|    ep_rew_mean      | 36.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 475      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 417      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 16.1     |\n",
      "|    n_updates        | 79       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.12     |\n",
      "|    ep_rew_mean      | 35.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 473      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 430      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 25.7     |\n",
      "|    n_updates        | 82       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.13     |\n",
      "|    ep_rew_mean      | 37       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 473      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 445      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 17.6     |\n",
      "|    n_updates        | 86       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.13     |\n",
      "|    ep_rew_mean      | 37       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 470      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 457      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 15.6     |\n",
      "|    n_updates        | 89       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.12     |\n",
      "|    ep_rew_mean      | 36.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 467      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 469      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 18.1     |\n",
      "|    n_updates        | 92       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.12     |\n",
      "|    ep_rew_mean      | 36.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 464      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 481      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 15.6     |\n",
      "|    n_updates        | 95       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.12     |\n",
      "|    ep_rew_mean      | 36.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 456      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 494      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 23.9     |\n",
      "|    n_updates        | 98       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.12     |\n",
      "|    ep_rew_mean      | 36.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 454      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 506      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 17.7     |\n",
      "|    n_updates        | 101      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.11     |\n",
      "|    ep_rew_mean      | 35       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 448      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 518      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 17       |\n",
      "|    n_updates        | 104      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.11     |\n",
      "|    ep_rew_mean      | 35       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 446      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 530      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 16       |\n",
      "|    n_updates        | 107      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.09     |\n",
      "|    ep_rew_mean      | 33.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 443      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 542      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 15.6     |\n",
      "|    n_updates        | 110      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.1      |\n",
      "|    ep_rew_mean      | 32.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 442      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 555      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 16.3     |\n",
      "|    n_updates        | 113      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.1      |\n",
      "|    ep_rew_mean      | 32.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 432      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 567      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 15       |\n",
      "|    n_updates        | 116      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.11     |\n",
      "|    ep_rew_mean      | 32.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 428      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 580      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 12.6     |\n",
      "|    n_updates        | 119      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.11     |\n",
      "|    ep_rew_mean      | 32.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 426      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 592      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 17.6     |\n",
      "|    n_updates        | 122      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.12     |\n",
      "|    ep_rew_mean      | 33.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 421      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 605      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 14.6     |\n",
      "|    n_updates        | 126      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.12     |\n",
      "|    ep_rew_mean      | 33.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 419      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 617      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 12.7     |\n",
      "|    n_updates        | 129      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.12     |\n",
      "|    ep_rew_mean      | 33.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 418      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 629      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 18       |\n",
      "|    n_updates        | 132      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.12     |\n",
      "|    ep_rew_mean      | 34.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 412      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 641      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 15.8     |\n",
      "|    n_updates        | 135      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.13     |\n",
      "|    ep_rew_mean      | 35.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 411      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 654      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 18       |\n",
      "|    n_updates        | 138      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.12     |\n",
      "|    ep_rew_mean      | 35.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 406      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 666      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 14.8     |\n",
      "|    n_updates        | 141      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.12     |\n",
      "|    ep_rew_mean      | 35.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 406      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 679      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 12.3     |\n",
      "|    n_updates        | 144      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.13     |\n",
      "|    ep_rew_mean      | 36.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 405      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 692      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.37     |\n",
      "|    n_updates        | 147      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.13     |\n",
      "|    ep_rew_mean      | 36.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 405      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 704      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 13.4     |\n",
      "|    n_updates        | 150      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.13     |\n",
      "|    ep_rew_mean      | 35       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 401      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 717      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 11       |\n",
      "|    n_updates        | 154      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.13     |\n",
      "|    ep_rew_mean      | 34       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 400      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 730      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 17       |\n",
      "|    n_updates        | 157      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.12     |\n",
      "|    ep_rew_mean      | 33.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 403      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 742      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 16.9     |\n",
      "|    n_updates        | 160      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.09     |\n",
      "|    ep_rew_mean      | 32.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 402      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 754      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 11.3     |\n",
      "|    n_updates        | 163      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.09     |\n",
      "|    ep_rew_mean      | 32.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 402      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 766      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 11.5     |\n",
      "|    n_updates        | 166      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.1      |\n",
      "|    ep_rew_mean      | 33.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 403      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 779      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 14.7     |\n",
      "|    n_updates        | 169      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.1      |\n",
      "|    ep_rew_mean      | 33.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 404      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 791      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.84     |\n",
      "|    n_updates        | 172      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.1      |\n",
      "|    ep_rew_mean      | 34.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 404      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 804      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.58     |\n",
      "|    n_updates        | 175      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.11     |\n",
      "|    ep_rew_mean      | 34.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 403      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 817      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 11.4     |\n",
      "|    n_updates        | 179      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.11     |\n",
      "|    ep_rew_mean      | 34.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 406      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 829      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.17     |\n",
      "|    n_updates        | 182      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.11     |\n",
      "|    ep_rew_mean      | 34.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 406      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 841      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 16.6     |\n",
      "|    n_updates        | 185      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.11     |\n",
      "|    ep_rew_mean      | 34.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 408      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 853      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 12       |\n",
      "|    n_updates        | 188      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.1      |\n",
      "|    ep_rew_mean      | 34.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 264      |\n",
      "|    fps              | 406      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 865      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 11.4     |\n",
      "|    n_updates        | 191      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.1      |\n",
      "|    ep_rew_mean      | 34.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 268      |\n",
      "|    fps              | 407      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 877      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 11.1     |\n",
      "|    n_updates        | 194      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.09     |\n",
      "|    ep_rew_mean      | 33.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 272      |\n",
      "|    fps              | 406      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 889      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 10.1     |\n",
      "|    n_updates        | 197      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.09     |\n",
      "|    ep_rew_mean      | 34.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 276      |\n",
      "|    fps              | 408      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 901      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 13       |\n",
      "|    n_updates        | 200      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.09     |\n",
      "|    ep_rew_mean      | 33.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 408      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 914      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 17.1     |\n",
      "|    n_updates        | 203      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.09     |\n",
      "|    ep_rew_mean      | 33.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 284      |\n",
      "|    fps              | 410      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 926      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 16.1     |\n",
      "|    n_updates        | 206      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.1      |\n",
      "|    ep_rew_mean      | 34.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 288      |\n",
      "|    fps              | 413      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 939      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 17.7     |\n",
      "|    n_updates        | 209      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.1      |\n",
      "|    ep_rew_mean      | 33.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 292      |\n",
      "|    fps              | 415      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 951      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 20.5     |\n",
      "|    n_updates        | 212      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.1      |\n",
      "|    ep_rew_mean      | 33.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 296      |\n",
      "|    fps              | 414      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 964      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 12.5     |\n",
      "|    n_updates        | 215      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.12     |\n",
      "|    ep_rew_mean      | 34.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 418      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 978      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.76     |\n",
      "|    n_updates        | 219      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.11     |\n",
      "|    ep_rew_mean      | 33.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 304      |\n",
      "|    fps              | 417      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 990      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.63     |\n",
      "|    n_updates        | 222      |\n",
      "----------------------------------\n",
      "Mean reward for PPO agent: 340.0 +/- 0.0\n",
      "Mean reward for DQN agent: 26.0 +/- 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20191678\\AppData\\Local\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the policy network architecture\n",
    "policy_kwargs = dict(activation_fn=torch.nn.Tanh, net_arch=[64, 64])\n",
    "\n",
    "# Training the PPO agent\n",
    "ppo_model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=1)\n",
    "ppo_model.learn(total_timesteps=TIME_STEPS)\n",
    "\n",
    "# Training the DQN agent\n",
    "dqn_model = DQN(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=1)\n",
    "dqn_model.learn(total_timesteps=TIME_STEPS)\n",
    "\n",
    "# Evaluating the PPO agentP\n",
    "mean_reward, std_reward = evaluate_policy(ppo_model, env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Mean reward for PPO agent: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "# Evaluating the DQN agent\n",
    "mean_reward, std_reward = evaluate_policy(dqn_model, env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Mean reward for DQN agent: {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with Different Neural Network Architectures\n",
    "Experiment with different neural network architectures for the DRL agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 4.99      |\n",
      "|    ep_rew_mean        | 290       |\n",
      "| time/                 |           |\n",
      "|    fps                | 242       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000402 |\n",
      "|    explained_variance | 0.518     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | 0.00137   |\n",
      "|    value_loss         | 3.77e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5         |\n",
      "|    ep_rew_mean        | 292       |\n",
      "| time/                 |           |\n",
      "|    fps                | 213       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000266 |\n",
      "|    explained_variance | 0.841     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 0.000779  |\n",
      "|    value_loss         | 1.93e+03  |\n",
      "-------------------------------------\n",
      "Mean reward for A2C agent with [128, 128] architecture: 292.0 +/- 0.0\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 3         |\n",
      "|    ep_rew_mean        | 172       |\n",
      "| time/                 |           |\n",
      "|    fps                | 148       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.97e-08 |\n",
      "|    explained_variance | 0.477     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 5.76e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 3         |\n",
      "|    ep_rew_mean        | 172       |\n",
      "| time/                 |           |\n",
      "|    fps                | 148       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.95e-08 |\n",
      "|    explained_variance | 0.92      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 1.52e+03  |\n",
      "-------------------------------------\n",
      "Mean reward for A2C agent with [256, 256] architecture: 172.0 +/- 0.0\n"
     ]
    }
   ],
   "source": [
    "# Define the policy network architecture\n",
    "policy_kwargs = dict(activation_fn=torch.nn.ReLU, net_arch=[128, 128])\n",
    "\n",
    "# Training the A2C agent with the new architecture\n",
    "a2c_model = A2C(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=1)\n",
    "a2c_model.learn(total_timesteps=TIME_STEPS)\n",
    "\n",
    "# Evaluating the A2C agent\n",
    "mean_reward, std_reward = evaluate_policy(a2c_model, env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Mean reward for A2C agent with [128, 128] architecture: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "# Define another policy network architecture\n",
    "policy_kwargs = dict(activation_fn=torch.nn.ReLU, net_arch=[256, 256])\n",
    "\n",
    "# Training the A2C agent with the new architecture\n",
    "a2c_model = A2C(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=1)\n",
    "a2c_model.learn(total_timesteps=TIME_STEPS)\n",
    "\n",
    "# Evaluating the A2C agent\n",
    "mean_reward, std_reward = evaluate_policy(a2c_model, env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Mean reward for A2C agent with [256, 256] architecture: {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune the Algorithms Hyperparameters\n",
    "Tune the hyperparameters of the algorithms by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.69     |\n",
      "|    ep_rew_mean     | 195      |\n",
      "| time/              |          |\n",
      "|    fps             | 195      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Mean reward for PPO agent after hyperparameter tuning: 172.0 +/- 0.0\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-------------------------------------------\n",
      "| rollout/            |                   |\n",
      "|    ep_len_mean      | 4.75              |\n",
      "|    ep_rew_mean      | 227               |\n",
      "|    exploration_rate | 0.819             |\n",
      "| time/               |                   |\n",
      "|    episodes         | 4                 |\n",
      "|    fps              | 85568392920039424 |\n",
      "|    time_elapsed     | 0                 |\n",
      "|    total_timesteps  | 19                |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/            |                    |\n",
      "|    ep_len_mean      | 4.5                |\n",
      "|    ep_rew_mean      | 188                |\n",
      "|    exploration_rate | 0.658              |\n",
      "| time/               |                    |\n",
      "|    episodes         | 8                  |\n",
      "|    fps              | 162129586585337856 |\n",
      "|    time_elapsed     | 0                  |\n",
      "|    total_timesteps  | 36                 |\n",
      "--------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.5      |\n",
      "|    ep_rew_mean      | 192      |\n",
      "|    exploration_rate | 0.487    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 3452     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 54       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.5      |\n",
      "|    ep_rew_mean      | 177      |\n",
      "|    exploration_rate | 0.316    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 4603     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 72       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.5      |\n",
      "|    ep_rew_mean      | 178      |\n",
      "|    exploration_rate | 0.145    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 5754     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 90       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.58     |\n",
      "|    ep_rew_mean      | 186      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 7033     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 110      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.57     |\n",
      "|    ep_rew_mean      | 191      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 3686     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 128      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.66     |\n",
      "|    ep_rew_mean      | 192      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 4290     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 149      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.75     |\n",
      "|    ep_rew_mean      | 203      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 3620     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 171      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.88     |\n",
      "|    ep_rew_mean      | 210      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 3780     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 195      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.86     |\n",
      "|    ep_rew_mean      | 204      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 3746     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 214      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.79     |\n",
      "|    ep_rew_mean      | 202      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 3828     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 230      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.83     |\n",
      "|    ep_rew_mean      | 199      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 3856     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 251      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.86     |\n",
      "|    ep_rew_mean      | 201      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 3638     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 272      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.82     |\n",
      "|    ep_rew_mean      | 197      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 3448     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 289      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.83     |\n",
      "|    ep_rew_mean      | 196      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 3364     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 309      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.81     |\n",
      "|    ep_rew_mean      | 196      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 3377     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 327      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.78     |\n",
      "|    ep_rew_mean      | 194      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 3318     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 344      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.79     |\n",
      "|    ep_rew_mean      | 193      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 3230     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 364      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.8      |\n",
      "|    ep_rew_mean      | 197      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 3199     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 384      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.79     |\n",
      "|    ep_rew_mean      | 196      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 3214     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 402      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.86     |\n",
      "|    ep_rew_mean      | 199      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 3290     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 428      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.84     |\n",
      "|    ep_rew_mean      | 197      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 3297     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 445      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.81     |\n",
      "|    ep_rew_mean      | 195      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 3277     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 462      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.82     |\n",
      "|    ep_rew_mean      | 197      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 3302     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 482      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.8      |\n",
      "|    ep_rew_mean      | 195      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 3295     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.84     |\n",
      "|    ep_rew_mean      | 197      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 3282     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 520      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.84     |\n",
      "|    ep_rew_mean      | 196      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 3333     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 538      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.86     |\n",
      "|    ep_rew_mean      | 200      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 3370     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 558      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.85     |\n",
      "|    ep_rew_mean      | 199      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 3280     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 575      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.85     |\n",
      "|    ep_rew_mean      | 200      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 3265     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 595      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.84     |\n",
      "|    ep_rew_mean      | 197      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 3249     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 612      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.83     |\n",
      "|    ep_rew_mean      | 198      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 3236     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 632      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.78     |\n",
      "|    ep_rew_mean      | 192      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 3240     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 649      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.74     |\n",
      "|    ep_rew_mean      | 190      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 3206     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 669      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.75     |\n",
      "|    ep_rew_mean      | 192      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 3276     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 689      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.81     |\n",
      "|    ep_rew_mean      | 194      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 3274     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 711      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.76     |\n",
      "|    ep_rew_mean      | 193      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 3330     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 727      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.73     |\n",
      "|    ep_rew_mean      | 190      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 3123     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 745      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.76     |\n",
      "|    ep_rew_mean      | 191      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 3129     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 765      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.72     |\n",
      "|    ep_rew_mean      | 189      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 3143     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 781      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.75     |\n",
      "|    ep_rew_mean      | 189      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 3111     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 802      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.76     |\n",
      "|    ep_rew_mean      | 191      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 3122     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 820      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.75     |\n",
      "|    ep_rew_mean      | 192      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 3158     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 839      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.74     |\n",
      "|    ep_rew_mean      | 188      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 3148     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 858      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.75     |\n",
      "|    ep_rew_mean      | 190      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 3210     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 877      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.69     |\n",
      "|    ep_rew_mean      | 189      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 3284     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 897      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.73     |\n",
      "|    ep_rew_mean      | 193      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 3235     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 918      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.74     |\n",
      "|    ep_rew_mean      | 194      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 3283     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 936      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.73     |\n",
      "|    ep_rew_mean      | 193      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 3350     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 955      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.73     |\n",
      "|    ep_rew_mean      | 193      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 3217     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 972      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.72     |\n",
      "|    ep_rew_mean      | 193      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 3283     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 992      |\n",
      "----------------------------------\n",
      "Mean reward for DQN agent after hyperparameter tuning: 172.0 +/- 0.0\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 3         |\n",
      "|    ep_rew_mean        | 32        |\n",
      "| time/                 |           |\n",
      "|    fps                | 107       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.63e-08 |\n",
      "|    explained_variance | 0.887     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 18.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 3         |\n",
      "|    ep_rew_mean        | 32        |\n",
      "| time/                 |           |\n",
      "|    fps                | 108       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.77e-08 |\n",
      "|    explained_variance | 0.998     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 0.643     |\n",
      "-------------------------------------\n",
      "Mean reward for A2C agent after hyperparameter tuning: 172.0 +/- 0.0\n"
     ]
    }
   ],
   "source": [
    "# Tuning the hyperparameters of the PPO agent\n",
    "ppo_model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=1, learning_rate=0.0003, n_steps=2048, batch_size=64, n_epochs=10, gamma=0.99, gae_lambda=0.95, clip_range=0.2)\n",
    "ppo_model.learn(total_timesteps=TIME_STEPS)\n",
    "\n",
    "# Evaluating the PPO agent\n",
    "mean_reward_ppo, std_reward_ppo = evaluate_policy(ppo_model, env, n_eval_episodes=10)\n",
    "print(f\"Mean reward for PPO agent after hyperparameter tuning: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "# Tuning the hyperparameters of the DQN agent\n",
    "dqn_model = DQN(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=1, learning_rate=0.0005, buffer_size=10000, learning_starts=1000, batch_size=64, tau=1.0, gamma=0.99, train_freq=4, gradient_steps=1)\n",
    "dqn_model.learn(total_timesteps=TIME_STEPS)\n",
    "\n",
    "# Evaluating the DQN agent\n",
    "mean_reward_dqn, std_reward_dqn = evaluate_policy(dqn_model, env, n_eval_episodes=10)\n",
    "print(f\"Mean reward for DQN agent after hyperparameter tuning: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "# Tuning the hyperparameters of the A2C agent\n",
    "a2c_model = A2C(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=1, learning_rate=0.0007, n_steps=5, gamma=0.99, gae_lambda=1.0, ent_coef=0.0, vf_coef=0.5, max_grad_norm=0.5, use_rms_prop=False, use_sde=False)\n",
    "a2c_model.learn(total_timesteps=TIME_STEPS)\n",
    "\n",
    "# Evaluating the A2C agent\n",
    "mean_reward_a2c, std_reward_a2c = evaluate_policy(a2c_model, env, n_eval_episodes=10)\n",
    "print(f\"Mean reward for A2C agent after hyperparameter tuning: {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Agents and Compare Results\n",
    "Evaluate the performance of the agents and compare the best results obtained using the different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the agents and their corresponding mean rewards and standard deviations\n",
    "agents = ['PPO', 'DQN', 'A2C']\n",
    "mean_rewards = [mean_reward_ppo, mean_reward_dqn, mean_reward_a2c]\n",
    "std_rewards = [std_reward_ppo, std_reward_dqn, std_reward_a2c]\n",
    "\n",
    "# Plotting the mean rewards\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(agents, mean_rewards, yerr=std_rewards, align='center', alpha=0.7, ecolor='black', capsize=10)\n",
    "plt.ylabel('Mean Reward')\n",
    "plt.title('Comparison of Mean Rewards of Different Agents')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is DQN with a mean reward of 142.0\n"
     ]
    }
   ],
   "source": [
    "# Define the agents and their corresponding mean rewards, standard deviations, and models\n",
    "agents = ['PPO', 'DQN', 'A2C']\n",
    "mean_rewards = [mean_reward_ppo, mean_reward_dqn, mean_reward_a2c]\n",
    "models = [ppo_model, dqn_model, a2c_model]\n",
    "\n",
    "# Determine the index of the best model\n",
    "best_index = np.argmax(mean_rewards)\n",
    "\n",
    "# Save the best model\n",
    "models[best_index].save(\"best_model_part1\")\n",
    "\n",
    "print(f\"The best model is {agents[best_index]} with a mean reward of {mean_rewards[best_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2\n",
    "\n",
    "---\n",
    "\n",
    "## Enabling action masking, train and test a MaskablePPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20191678\\AppData\\Local\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\sb3_contrib\\common\\maskable\\policies.py:78: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n",
      "c:\\Users\\20191678\\AppData\\Local\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.action_masks to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.action_masks` for environment variables or `env.get_wrapper_attr('action_masks')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.32     |\n",
      "|    ep_rew_mean     | 385      |\n",
      "| time/              |          |\n",
      "|    fps             | 120      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Mean reward: 616.0 +/- 0.0\n"
     ]
    }
   ],
   "source": [
    "# Define a function for the mask\n",
    "def mask_fn(env):\n",
    "    return env.get_mask()\n",
    "\n",
    "# Create an instance of the environment with mask enabled\n",
    "env = BoundedKnapsackEnv(n_items=200, max_weight=200, mask=True)\n",
    "\n",
    "# Wrap the environment with the ActionMasker\n",
    "vec_env = ActionMasker(env, mask_fn)\n",
    "\n",
    "# Define the policy architecture\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi=[128, 128, 128], vf=[128, 128, 128])],  \n",
    ")\n",
    "\n",
    "# Train a MaskablePPO agent\n",
    "model = MaskablePPO(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=1,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    learning_rate=3e-4,\n",
    "    ent_coef=0.0,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    target_kl=None,\n",
    ")\n",
    "\n",
    "# Adjust timesteps for meaningful training\n",
    "model.learn(total_timesteps=TIME_STEPS, use_masking=True)\n",
    "\n",
    "# Evaluate the agent\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with different neural network architectures and tune the algorithm hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.67     |\n",
      "|    ep_rew_mean     | 390      |\n",
      "| time/              |          |\n",
      "|    fps             | 137      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Architecture: [{'pi': [64, 64], 'vf': [64, 64]}], Learning Rate: 0.001, Mean reward: 616.0 +/- 0.0\n",
      "New best mean reward: 616.0 +/- 0.0, model saved.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.36     |\n",
      "|    ep_rew_mean     | 382      |\n",
      "| time/              |          |\n",
      "|    fps             | 122      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Architecture: [{'pi': [64, 64], 'vf': [64, 64]}], Learning Rate: 0.0003, Mean reward: 616.0 +/- 0.0\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.39     |\n",
      "|    ep_rew_mean     | 363      |\n",
      "| time/              |          |\n",
      "|    fps             | 150      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Architecture: [{'pi': [64, 64], 'vf': [64, 64]}], Learning Rate: 0.0001, Mean reward: 616.0 +/- 0.0\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.46     |\n",
      "|    ep_rew_mean     | 375      |\n",
      "| time/              |          |\n",
      "|    fps             | 165      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Architecture: [{'pi': [128, 128], 'vf': [128, 128]}], Learning Rate: 0.001, Mean reward: 616.0 +/- 0.0\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.26     |\n",
      "|    ep_rew_mean     | 371      |\n",
      "| time/              |          |\n",
      "|    fps             | 153      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Architecture: [{'pi': [128, 128], 'vf': [128, 128]}], Learning Rate: 0.0003, Mean reward: 616.0 +/- 0.0\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.54     |\n",
      "|    ep_rew_mean     | 381      |\n",
      "| time/              |          |\n",
      "|    fps             | 126      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Architecture: [{'pi': [128, 128], 'vf': [128, 128]}], Learning Rate: 0.0001, Mean reward: 616.0 +/- 0.0\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.47     |\n",
      "|    ep_rew_mean     | 377      |\n",
      "| time/              |          |\n",
      "|    fps             | 137      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Architecture: [{'pi': [256, 256], 'vf': [256, 256]}], Learning Rate: 0.001, Mean reward: 616.0 +/- 0.0\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.59     |\n",
      "|    ep_rew_mean     | 395      |\n",
      "| time/              |          |\n",
      "|    fps             | 119      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Architecture: [{'pi': [256, 256], 'vf': [256, 256]}], Learning Rate: 0.0003, Mean reward: 616.0 +/- 0.0\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.93     |\n",
      "|    ep_rew_mean     | 414      |\n",
      "| time/              |          |\n",
      "|    fps             | 117      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Architecture: [{'pi': [256, 256], 'vf': [256, 256]}], Learning Rate: 0.0001, Mean reward: 616.0 +/- 0.0\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.43     |\n",
      "|    ep_rew_mean     | 383      |\n",
      "| time/              |          |\n",
      "|    fps             | 149      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Architecture: [{'pi': [128, 64], 'vf': [128, 64]}], Learning Rate: 0.001, Mean reward: 616.0 +/- 0.0\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.58     |\n",
      "|    ep_rew_mean     | 385      |\n",
      "| time/              |          |\n",
      "|    fps             | 155      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Architecture: [{'pi': [128, 64], 'vf': [128, 64]}], Learning Rate: 0.0003, Mean reward: 616.0 +/- 0.0\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.78     |\n",
      "|    ep_rew_mean     | 407      |\n",
      "| time/              |          |\n",
      "|    fps             | 106      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Architecture: [{'pi': [128, 64], 'vf': [128, 64]}], Learning Rate: 0.0001, Mean reward: 616.0 +/- 0.0\n"
     ]
    }
   ],
   "source": [
    "# Define a list of architectures to experiment with\n",
    "architectures = [\n",
    "    [dict(pi=[64, 64], vf=[64, 64])],\n",
    "    [dict(pi=[128, 128], vf=[128, 128])],\n",
    "    [dict(pi=[256, 256], vf=[256, 256])],\n",
    "    [dict(pi=[128, 64], vf=[128, 64])],\n",
    "]\n",
    "\n",
    "# Define a list of learning rates to experiment with\n",
    "learning_rates = [1e-3, 3e-4, 1e-4]\n",
    "\n",
    "# Initialize best mean reward to negative infinity\n",
    "best_mean_reward = -np.inf\n",
    "\n",
    "# Loop over the architectures and learning rates\n",
    "for arch in architectures:\n",
    "    for lr in learning_rates:\n",
    "        policy_kwargs = dict(net_arch=arch)\n",
    "        model = MaskablePPO(\n",
    "            \"MlpPolicy\",\n",
    "            vec_env,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            verbose=1,\n",
    "            n_steps=2048,\n",
    "            batch_size=64,\n",
    "            learning_rate=lr,\n",
    "            ent_coef=0.0,\n",
    "            gamma=0.99,\n",
    "            gae_lambda=0.95,\n",
    "            clip_range=0.2,\n",
    "            vf_coef=0.5,\n",
    "            max_grad_norm=0.5,\n",
    "            target_kl=None,\n",
    "        )\n",
    "        model.learn(total_timesteps=TIME_STEPS, use_masking=True)\n",
    "        mean_reward_mppo, std_reward_mppo = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "        print(f\"Architecture: {arch}, Learning Rate: {lr}, Mean reward: {mean_reward} +/- {std_reward}\")\n",
    "        \n",
    "        # If this mean reward is greater than the current best, save this model\n",
    "        if mean_reward_mppo > best_mean_reward:\n",
    "            best_mean_reward = mean_reward_mppo\n",
    "            model.save(\"best_model_part2\")\n",
    "            print(f\"New best mean reward: {mean_reward_mppo} +/- {std_reward_mppo}, model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the agent and compare the best results obtained with those of the best agent from Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Part 1 - Mean reward: 224.0 +/- 0.0\n",
      "Part 2 - Mean reward: 616.0 +/- 0.0\n"
     ]
    }
   ],
   "source": [
    "# Load the best model from Part 1\n",
    "if agents[best_index] == 'PPO':\n",
    "    best_model_part1 = PPO.load(\"best_model_part1\", env=env)\n",
    "elif agents[best_index] == 'DQN':\n",
    "    best_model_part1 = DQN.load(\"best_model_part1\", env=env)\n",
    "elif agents[best_index] == 'A2C':\n",
    "    best_model_part1 = A2C.load(\"best_model_part1\", env=env)\n",
    "else:\n",
    "    print(\"Unknown model type\")\n",
    "    \n",
    "# Load the best model from Part 2\n",
    "best_model_part2 = MaskablePPO.load(\"best_model_part2\", env=env)\n",
    "\n",
    "# Evaluate the best model from Part 1\n",
    "mean_reward_part1, std_reward_part1 = evaluate_policy(best_model_part1, env, n_eval_episodes=100)\n",
    "\n",
    "# Evaluate the best model from Part 2\n",
    "mean_reward_part2, std_reward_part2 = evaluate_policy(best_model_part2, env, n_eval_episodes=100)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Part 1 - Mean reward: {mean_reward_part1} +/- {std_reward_part1}\")\n",
    "print(f\"Part 2 - Mean reward: {mean_reward_part2} +/- {std_reward_part2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
