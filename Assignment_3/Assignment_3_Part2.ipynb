{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries, including SB3 Contrib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import gymnasium\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.envs import BitFlippingEnv\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from sb3_contrib import MaskablePPO\n",
    "from knapsack_env import BoundedKnapsackEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the BoundedKnapsack Environment\n",
    "Define the BoundedKnapsack environment and initialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the BoundedKnapsack environment\n",
    "env = BoundedKnapsackEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enable Action Masking\n",
    "Enable action masking in the environment using the get_mask() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DummyVecEnv' object has no attribute 'get_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m MaskablePPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Enable action masking\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m env\u001b[38;5;241m.\u001b[39maction_mask \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munwrapped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mask\u001b[49m()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DummyVecEnv' object has no attribute 'get_mask'"
     ]
    }
   ],
   "source": [
    "# Define the MaskablePPO model\n",
    "model = MaskablePPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Enable action masking\n",
    "env.action_mask = env.get_mask()\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a MaskablePPO Agent\n",
    "Train a MaskablePPO agent from SB3 Contrib on the BoundedKnapsack environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "\n",
    "# Print the mean reward\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "# Compare the results with the best agent from Part 1\n",
    "# Assuming the best reward from Part 1 is stored in a variable `best_reward_part1`\n",
    "if mean_reward > best_reward_part1:\n",
    "    print(\"The MaskablePPO agent performed better than the best agent from Part 1.\")\n",
    "else:\n",
    "    print(\"The best agent from Part 1 performed better than the MaskablePPO agent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with Different Neural Network Architectures\n",
    "Experiment with different neural network architectures for the MaskablePPO agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for neural network\n",
    "from stable_baselines3.ppo.policies import MlpPolicy, CnnPolicy\n",
    "\n",
    "# List of different neural network architectures to experiment with\n",
    "policies = [MlpPolicy, CnnPolicy]\n",
    "\n",
    "# Dictionary to store the mean rewards for each policy\n",
    "rewards = {}\n",
    "\n",
    "# Loop through each policy\n",
    "for policy in policies:\n",
    "    # Define the MaskablePPO model with the current policy\n",
    "    model = MaskablePPO(policy, env, verbose=1)\n",
    "\n",
    "    # Train the model\n",
    "    model.learn(total_timesteps=10000)\n",
    "\n",
    "    # Evaluate the trained model\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "\n",
    "    # Store the mean reward in the dictionary\n",
    "    rewards[policy.__name__] = mean_reward\n",
    "\n",
    "    # Print the mean reward for the current policy\n",
    "    print(f\"Mean reward for {policy.__name__}: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "# Print the policy with the highest mean reward\n",
    "best_policy = max(rewards, key=rewards.get)\n",
    "print(f\"The best policy is {best_policy} with a mean reward of {rewards[best_policy]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune the Algorithm Hyperparameters\n",
    "Manually tune the algorithm hyperparameters to improve the agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters to tune\n",
    "hyperparams = {\n",
    "    'n_steps': [128, 256, 512],\n",
    "    'gamma': [0.9, 0.99, 0.999],\n",
    "    'learning_rate': [0.0001, 0.001, 0.01],\n",
    "}\n",
    "\n",
    "# Dictionary to store the mean rewards for each set of hyperparameters\n",
    "rewards = {}\n",
    "\n",
    "# Loop through each set of hyperparameters\n",
    "for n_steps in hyperparams['n_steps']:\n",
    "    for gamma in hyperparams['gamma']:\n",
    "        for learning_rate in hyperparams['learning_rate']:\n",
    "            # Define the MaskablePPO model with the current set of hyperparameters\n",
    "            model = MaskablePPO(best_policy, env, verbose=1, n_steps=n_steps, gamma=gamma, learning_rate=learning_rate)\n",
    "\n",
    "            # Train the model\n",
    "            model.learn(total_timesteps=10000)\n",
    "\n",
    "            # Evaluate the trained model\n",
    "            mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "\n",
    "            # Store the mean reward in the dictionary\n",
    "            rewards[(n_steps, gamma, learning_rate)] = mean_reward\n",
    "\n",
    "            # Print the mean reward for the current set of hyperparameters\n",
    "            print(f\"Mean reward for n_steps={n_steps}, gamma={gamma}, learning_rate={learning_rate}: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "# Print the set of hyperparameters with the highest mean reward\n",
    "best_hyperparams = max(rewards, key=rewards.get)\n",
    "print(f\"The best hyperparameters are n_steps={best_hyperparams[0]}, gamma={best_hyperparams[1]}, learning_rate={best_hyperparams[2]} with a mean reward of {rewards[best_hyperparams]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Agent\n",
    "Evaluate the trained agent's performance over 100 episodes and calculate the average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model with the best policy and hyperparameters\n",
    "model = MaskablePPO(best_policy, env, verbose=1, n_steps=best_hyperparams[0], gamma=best_hyperparams[1], learning_rate=best_hyperparams[2])\n",
    "model.learn(total_timesteps=10000)\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "\n",
    "# Print the mean reward\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "# Compare the results with the best agent from Part 1\n",
    "if mean_reward > best_reward_part1:\n",
    "    print(\"The MaskablePPO agent performed better than the best agent from Part 1.\")\n",
    "else:\n",
    "    print(\"The best agent from Part 1 performed better than the MaskablePPO agent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Results with Part 1\n",
    "Compare the results of the MaskablePPO agent with the best agent from Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the results with the best agent from Part 1\n",
    "best_reward_part2 = mean_reward\n",
    "\n",
    "if best_reward_part2 > best_reward_part1:\n",
    "    print(\"The MaskablePPO agent performed better than the best agent from Part 1.\")\n",
    "else:\n",
    "    print(\"The best agent from Part 1 performed better than the MaskablePPO agent.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
