{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Libraries\n",
    "Install the necessary libraries, including Gymnasium, Stable Baselines3, and SB3 Contrib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required modules\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from sb3_contrib.common.maskable.evaluation import evaluate_policy as evaluate_policy_maskable\n",
    "import optuna\n",
    "\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "\n",
    "# Import BoundedKnapsack Environment\n",
    "from knapsack_env import BoundedKnapsackEnv\n",
    "\n",
    "# Setting the seed for reproducibility\n",
    "seed = 2024\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Create a log directory\n",
    "log_dir = './logs/'\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global variables\n",
    "TIME_STEPS = 10000\n",
    "EVAL_EPISODES = 100   \n",
    "EVAL_FREQ = int(TIME_STEPS**0.5)     \n",
    "VERBOSE = 0\n",
    "N_TRIALS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an Instance of the Environment\n",
    "Create an instance of the BoundedKnapsack environment with the specified parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Space: (array([[ 82,  65,  60,  69,  35,  87,  35,  48,  40,  23,  15,  86,   3,\n",
      "         41,  26,  64,  42,  52,  95,  67,  17,  45,  93,  30,  53,  50,\n",
      "         20,  74,  49,  66,   5,  48,  51,  19,  71,  51,  10,  93,  44,\n",
      "         55,  28,  93,  31,  37,  88,  30,   6,   3,   9,   9,  37,  15,\n",
      "         38,  83,  39,  26,  33,  98,  69,  82,  25,  90,  18,  57,  95,\n",
      "         95,  60,  65,  38,  86,  57,  80,  75,  70,  10,  19,  20,  66,\n",
      "         57,  42,  36,  30,  63,  35,  22,  34,  59,  80,  48,  56,  70,\n",
      "         55,  20,  21,  83,  57,  43,  66,  71,  79,   3,  99,   7,  54,\n",
      "         58,  79,  69,  43,  40,   6,  34,  73,  15,  19,  32,  82,  31,\n",
      "          9,   2,  31,  54,  83,  68,  44,  59,  41,  63,  28,  95,  48,\n",
      "         36,  98,  91,  71,  39,  31,  60,  11,  33,  10,  94,  93,  30,\n",
      "         36,  34,  20,  99,  80,  85,  89,  49,  79,  85,  76,   3,  50,\n",
      "         63,  11,  83,  40,  23,  68,  29,  55,  43,  12,  28,  21,  52,\n",
      "         52,  44,  45,  46,  93,  24,  26,  26,  17,  59,  62,  98,  71,\n",
      "         30,  21,  76,  46,  80,   9,  69,  93,  95,  15,  68,  36,  86,\n",
      "         43,  66,  64,  66,  10, 200],\n",
      "       [ 89,  97,  29,  59,  83,  95,   2,   6,  33,  55,  75,  36,   5,\n",
      "          9,  75,  25,  40,  94,  71,  15,  21,  77,   7,  61,  81,  93,\n",
      "         85,  30,  47,  14,  85,  18,  27,  92,   8,  54,  63,  37,  67,\n",
      "         98,  37,  86,  30,   3,  50,  55,  80,  36,  74,  53,   3,  31,\n",
      "         28,  40,  64,  44,  41,  61,  96,  38,  75,  15,  21,  59,  16,\n",
      "         94,  94,  34,  86,  30,  79,  78,  45,  97,  88,  70,  19,  36,\n",
      "          5,  24,  42,  52,  64,  63,  85,  11,  72,  73,  10,  82,  15,\n",
      "          0,  90,  73,  21,   4,  24,  96,  23,  86,  38,   6,  69,  43,\n",
      "         44,  43,  49,  57,  98,  79,  54,  78,  57,   2,  83,  93,  76,\n",
      "         27,  85,  26,  92,  52,  28,  58,  76,  31,  66,  66,  83,  83,\n",
      "         54,  44,  72,  72,  76,  48,  42,  53,  84,  58,  23,  45,  17,\n",
      "         37,   5,  58,  19,  92,  86,  90,  23,  73,   9,  13,   0,  79,\n",
      "         61,  62,  91,  24,  14,  97,  81,  94,  73,  54,  54,   4,  86,\n",
      "         80,  71,  60,  90,  62,  51,  96,  49,  63,  87,  90,  90,  17,\n",
      "          8,  28,  69,   9,  72,  31,  72,  34,   6,  52,  49,  50,  42,\n",
      "         35,  82,  40,   5,   8,   0],\n",
      "       [  5,   8,   3,   5,   3,   6,   6,   1,   9,   9,   2,   9,   8,\n",
      "          2,   5,   9,   1,   3,   3,   4,   4,   1,   4,   2,   7,   7,\n",
      "          5,   2,   9,   3,   4,   2,   4,   6,   2,   8,   5,   3,   3,\n",
      "          1,   3,   4,   1,   9,   4,   3,   3,   5,   5,   4,   3,   9,\n",
      "          9,   5,   3,   7,   7,   7,   2,   3,   9,   1,   3,   5,   8,\n",
      "          5,   1,   8,   6,   8,   5,   1,   4,   4,   1,   5,   5,   5,\n",
      "          3,   1,   7,   9,   8,   4,   7,   5,   6,   3,   2,   1,   3,\n",
      "          3,   8,   2,   9,   6,   2,   6,   1,   9,   4,   5,   1,   4,\n",
      "          4,   1,   2,   8,   1,   5,   9,   3,   5,   2,   1,   2,   2,\n",
      "          9,   3,   6,   1,   1,   3,   8,   1,   8,   5,   3,   5,   7,\n",
      "          1,   6,   3,   2,   3,   8,   5,   9,   7,   1,   6,   6,   6,\n",
      "          3,   3,   4,   4,   8,   2,   3,   4,   6,   3,   3,   8,   1,\n",
      "          5,   3,   6,   6,   6,   5,   2,   4,   5,   9,   5,   8,   2,\n",
      "          6,   6,   8,   8,   7,   9,   6,   4,   4,   5,   4,   2,   7,\n",
      "          9,   8,   3,   8,   8,   6,   3,   4,   9,   4,   9,   1,   6,\n",
      "          3,   1,   8,   9,   7,   0]]), {})\n",
      "Action Space Size: 200\n"
     ]
    }
   ],
   "source": [
    "# Enable the environment\n",
    "env = BoundedKnapsackEnv(n_items=200, max_weight=200)\n",
    "\n",
    "# Create evaluation environment\n",
    "eval_env = Monitor(env)\n",
    "\n",
    "# Inspect the state space and action spaces\n",
    "state_space = env.reset()\n",
    "action_space_size = env.action_space.n\n",
    "\n",
    "# Print the state space and action space size\n",
    "print(f\"State Space: {state_space}\")\n",
    "print(f\"Action Space Size: {action_space_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test DRL Agents\n",
    "Train and test at least two different DRL agents using the algorithms provided in Stable Baselines3 with default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward for PPO agent: 255.0 +/- 0.0\n",
      "Mean reward for DQN agent: 172.0 +/- 0.0\n"
     ]
    }
   ],
   "source": [
    "# Define the policy network architecture\n",
    "policy_kwargs = dict(activation_fn=torch.nn.Tanh, net_arch=[64, 64])\n",
    "\n",
    "# Training the PPO agent\n",
    "ppo_model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=VERBOSE)\n",
    "ppo_model.learn(total_timesteps=TIME_STEPS)\n",
    "\n",
    "# Training the DQN agent\n",
    "dqn_model = DQN(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=VERBOSE, buffer_size=TIME_STEPS//10)\n",
    "dqn_model.learn(total_timesteps=TIME_STEPS)\n",
    "\n",
    "# Evaluating the PPO agent\n",
    "mean_reward_ppo, std_reward_ppo = evaluate_policy(ppo_model, eval_env, n_eval_episodes=EVAL_EPISODES, deterministic=True)\n",
    "print(f\"Mean reward for PPO agent: {mean_reward_ppo} +/- {std_reward_ppo}\")\n",
    "\n",
    "# Evaluating the DQN agent\n",
    "mean_reward_dqn, std_reward_dqn = evaluate_policy(dqn_model, eval_env, n_eval_episodes=EVAL_EPISODES, deterministic=True)\n",
    "print(f\"Mean reward for DQN agent: {mean_reward_dqn} +/- {std_reward_dqn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with Different Neural Network Architectures\n",
    "Experiment with different neural network architectures for the DRL agents with default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward for PPO agent: 552.0 +/- 0.0\n",
      "Mean reward for DQN agent: 267.0 +/- 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the policy network architecture\n",
    "policy_kwargs = dict(activation_fn=torch.nn.ReLU, net_arch=[128, 128])\n",
    "\n",
    "# Training the PPO agent\n",
    "ppo_model_large = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=VERBOSE)\n",
    "ppo_model_large.learn(total_timesteps=TIME_STEPS)\n",
    "\n",
    "# Training the DQN agent\n",
    "dqn_model_large = DQN(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=VERBOSE, buffer_size=TIME_STEPS//10)\n",
    "dqn_model_large.learn(total_timesteps=TIME_STEPS)\n",
    "\n",
    "# Evaluating the PPO agent\n",
    "mean_reward, std_reward = evaluate_policy(ppo_model_large, eval_env, n_eval_episodes=EVAL_EPISODES, deterministic=True)\n",
    "print(f\"Mean reward for PPO agent: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "# Evaluating the DQN agent\n",
    "mean_reward, std_reward = evaluate_policy(dqn_model_large, eval_env, n_eval_episodes=EVAL_EPISODES, deterministic=True)\n",
    "print(f\"Mean reward for DQN agent: {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune the Algorithms Hyperparameters\n",
    "Tune the hyperparameters of the algorithms by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-25 08:10:44,580] A new study created in memory with name: no-name-bf3dba98-22ea-4117-98b0-c12fb1b04cc8\n",
      "c:\\Users\\20191678\\AppData\\Local\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:155: UserWarning: You have specified a mini-batch size of 32, but because the `RolloutBuffer` is of size `n_steps * n_envs = 3781`, after every 118 untruncated mini-batches, there will be a truncated mini-batch of size 5\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=3781 and n_envs=1)\n",
      "  warnings.warn(\n",
      "[I 2024-06-25 08:11:27,150] Trial 0 finished with value: 340.0 and parameters: {'learning_rate': 4.5517621281043435e-05, 'n_steps': 3781, 'batch_size': 32, 'n_epochs': 4, 'gamma': 0.9309411363992524, 'gae_lambda': 0.8375085201139674, 'clip_range': 0.33000851830080435, 'ent_coef': 0.0026775701672321415}. Best is trial 0 with value: 340.0.\n",
      "c:\\Users\\20191678\\AppData\\Local\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:155: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2455`, after every 38 untruncated mini-batches, there will be a truncated mini-batch of size 23\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2455 and n_envs=1)\n",
      "  warnings.warn(\n",
      "[I 2024-06-25 08:12:17,061] Trial 1 finished with value: 552.0 and parameters: {'learning_rate': 2.200842470328139e-05, 'n_steps': 2455, 'batch_size': 64, 'n_epochs': 10, 'gamma': 0.9615801151982589, 'gae_lambda': 0.9848794312402013, 'clip_range': 0.26406046449497567, 'ent_coef': 0.0008429202916459421}. Best is trial 1 with value: 552.0.\n",
      "c:\\Users\\20191678\\AppData\\Local\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:155: UserWarning: You have specified a mini-batch size of 32, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2877`, after every 89 untruncated mini-batches, there will be a truncated mini-batch of size 29\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2877 and n_envs=1)\n",
      "  warnings.warn(\n",
      "[I 2024-06-25 08:13:08,076] Trial 2 finished with value: 395.0 and parameters: {'learning_rate': 0.00043638901066418576, 'n_steps': 2877, 'batch_size': 32, 'n_epochs': 6, 'gamma': 0.9395557787442566, 'gae_lambda': 0.83238687335609, 'clip_range': 0.13724062431767223, 'ent_coef': 8.684012161016443e-05}. Best is trial 1 with value: 552.0.\n",
      "c:\\Users\\20191678\\AppData\\Local\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:155: UserWarning: You have specified a mini-batch size of 32, but because the `RolloutBuffer` is of size `n_steps * n_envs = 3470`, after every 108 untruncated mini-batches, there will be a truncated mini-batch of size 14\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=3470 and n_envs=1)\n",
      "  warnings.warn(\n",
      "[I 2024-06-25 08:13:56,531] Trial 3 finished with value: 552.0 and parameters: {'learning_rate': 0.000471819409323207, 'n_steps': 3470, 'batch_size': 32, 'n_epochs': 5, 'gamma': 0.9421871991395376, 'gae_lambda': 0.8249242727225911, 'clip_range': 0.37346664073858327, 'ent_coef': 0.00024405749873257496}. Best is trial 1 with value: 552.0.\n",
      "c:\\Users\\20191678\\AppData\\Local\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:155: UserWarning: You have specified a mini-batch size of 128, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2406`, after every 18 untruncated mini-batches, there will be a truncated mini-batch of size 102\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2406 and n_envs=1)\n",
      "  warnings.warn(\n",
      "[I 2024-06-25 08:14:40,892] Trial 4 finished with value: 285.0 and parameters: {'learning_rate': 0.0006561132437388084, 'n_steps': 2406, 'batch_size': 128, 'n_epochs': 5, 'gamma': 0.9622858390576409, 'gae_lambda': 0.990373678820849, 'clip_range': 0.32960640930821755, 'ent_coef': 1.214253503983981e-05}. Best is trial 1 with value: 552.0.\n",
      "[I 2024-06-25 08:15:24,049] Trial 5 finished with value: 491.0 and parameters: {'learning_rate': 0.00010619395046147744, 'n_steps': 2176, 'batch_size': 64, 'n_epochs': 7, 'gamma': 0.9768109992721543, 'gae_lambda': 0.891271288601554, 'clip_range': 0.27226337404497136, 'ent_coef': 1.3320866778545666e-05}. Best is trial 1 with value: 552.0.\n",
      "c:\\Users\\20191678\\AppData\\Local\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:155: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2632`, after every 41 untruncated mini-batches, there will be a truncated mini-batch of size 8\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2632 and n_envs=1)\n",
      "  warnings.warn(\n",
      "[I 2024-06-25 08:16:35,281] Trial 6 finished with value: 370.0 and parameters: {'learning_rate': 0.00017082735849271451, 'n_steps': 2632, 'batch_size': 64, 'n_epochs': 10, 'gamma': 0.9820400244546703, 'gae_lambda': 0.8918877891770416, 'clip_range': 0.2703872457966323, 'ent_coef': 0.000325731046074857}. Best is trial 1 with value: 552.0.\n",
      "c:\\Users\\20191678\\AppData\\Local\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:155: UserWarning: You have specified a mini-batch size of 32, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2262`, after every 70 untruncated mini-batches, there will be a truncated mini-batch of size 22\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2262 and n_envs=1)\n",
      "  warnings.warn(\n",
      "[I 2024-06-25 08:17:30,123] Trial 7 finished with value: 340.0 and parameters: {'learning_rate': 1.4324581278405518e-05, 'n_steps': 2262, 'batch_size': 32, 'n_epochs': 6, 'gamma': 0.9049777757326911, 'gae_lambda': 0.85950677728544, 'clip_range': 0.36585732681449856, 'ent_coef': 2.2484994883617397e-05}. Best is trial 1 with value: 552.0.\n",
      "c:\\Users\\20191678\\AppData\\Local\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:155: UserWarning: You have specified a mini-batch size of 32, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2049`, after every 64 untruncated mini-batches, there will be a truncated mini-batch of size 1\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2049 and n_envs=1)\n",
      "  warnings.warn(\n",
      "[I 2024-06-25 08:18:25,759] Trial 8 finished with value: 693.0 and parameters: {'learning_rate': 0.00016854980280065515, 'n_steps': 2049, 'batch_size': 32, 'n_epochs': 8, 'gamma': 0.9783950352040678, 'gae_lambda': 0.8993043992247972, 'clip_range': 0.12234397626241642, 'ent_coef': 1.191818873356803e-05}. Best is trial 8 with value: 693.0.\n",
      "c:\\Users\\20191678\\AppData\\Local\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:155: UserWarning: You have specified a mini-batch size of 128, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2901`, after every 22 untruncated mini-batches, there will be a truncated mini-batch of size 85\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2901 and n_envs=1)\n",
      "  warnings.warn(\n",
      "[I 2024-06-25 08:18:59,534] Trial 9 finished with value: 395.0 and parameters: {'learning_rate': 1.8380392036544714e-05, 'n_steps': 2901, 'batch_size': 128, 'n_epochs': 5, 'gamma': 0.9909210111733276, 'gae_lambda': 0.966285458281777, 'clip_range': 0.17663381426455316, 'ent_coef': 0.027830169810043954}. Best is trial 8 with value: 693.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for PPO: {'learning_rate': 0.00016854980280065515, 'n_steps': 2049, 'batch_size': 32, 'n_epochs': 8, 'gamma': 0.9783950352040678, 'gae_lambda': 0.8993043992247972, 'clip_range': 0.12234397626241642, 'ent_coef': 1.191818873356803e-05}\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function for PPO\n",
    "def ppo_objective(trial):\n",
    "    policy_kwargs = dict(\n",
    "        activation_fn=torch.nn.ReLU,\n",
    "        net_arch=[128, 128]\n",
    "    )\n",
    "    \n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    n_steps = trial.suggest_int('n_steps', 2048, 4096)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    n_epochs = trial.suggest_int('n_epochs', 3, 10)\n",
    "    gamma = trial.suggest_float('gamma', 0.9, 0.9999)\n",
    "    gae_lambda = trial.suggest_float('gae_lambda', 0.8, 1.0)\n",
    "    clip_range = trial.suggest_float('clip_range', 0.1, 0.4)\n",
    "    ent_coef = trial.suggest_float('ent_coef', 0.00001, 0.1, log=True)\n",
    "    \n",
    "    # Create the PPO model\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\", \n",
    "        env, \n",
    "        policy_kwargs=policy_kwargs, \n",
    "        verbose=0,\n",
    "        learning_rate=learning_rate, \n",
    "        n_steps=n_steps, \n",
    "        batch_size=batch_size, \n",
    "        n_epochs=n_epochs, \n",
    "        gamma=gamma, \n",
    "        gae_lambda=gae_lambda, \n",
    "        clip_range=clip_range, \n",
    "        ent_coef=ent_coef,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.learn(total_timesteps=TIME_STEPS)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    mean_reward, _ = evaluate_policy(model, eval_env, n_eval_episodes=EVAL_EPISODES)\n",
    "    \n",
    "    return mean_reward\n",
    "\n",
    "# Optimize the hyperparameters\n",
    "ppo_study = optuna.create_study(direction='maximize')\n",
    "ppo_study.optimize(ppo_objective, n_trials=N_TRIALS)\n",
    "\n",
    "print('Best hyperparameters for PPO:', ppo_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best PPO model\n",
    "best_params = ppo_study.best_params\n",
    "\n",
    "ppo_model_best = PPO(\n",
    "    \"MlpPolicy\", \n",
    "    env, \n",
    "    policy_kwargs=policy_kwargs, \n",
    "    verbose=VERBOSE,\n",
    "    learning_rate=best_params['learning_rate'], \n",
    "    n_steps=best_params['n_steps'], \n",
    "    batch_size=best_params['batch_size'], \n",
    "    n_epochs=best_params['n_epochs'], \n",
    "    gamma=best_params['gamma'], \n",
    "    gae_lambda=best_params['gae_lambda'], \n",
    "    clip_range=best_params['clip_range'], \n",
    "    ent_coef=best_params['ent_coef'],\n",
    "    tensorboard_log=log_dir + 'ppo'\n",
    ")\n",
    "ppo_model_best.learn(total_timesteps=TIME_STEPS)\n",
    "\n",
    "# Save the model\n",
    "ppo_model_best.save(\"models/ppo_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-25 08:19:53,228] A new study created in memory with name: no-name-291432bd-b104-4389-b8fe-e3d4ac9e125a\n",
      "[I 2024-06-25 08:20:08,588] Trial 0 finished with value: 146.0 and parameters: {'learning_rate': 6.633768011392626e-05, 'buffer_size': 1991, 'learning_starts': 2392, 'batch_size': 64, 'exploration_initial_eps': 0.9752717890435186, 'exploration_final_eps': 0.0752582216945292, 'exploration_fraction': 0.1582589708136582, 'train_freq': 8, 'gamma': 0.9559970491033192, 'target_update_interval': 2805}. Best is trial 0 with value: 146.0.\n",
      "[I 2024-06-25 08:20:52,780] Trial 1 finished with value: 291.0 and parameters: {'learning_rate': 1.566632687896348e-05, 'buffer_size': 7136, 'learning_starts': 3177, 'batch_size': 128, 'exploration_initial_eps': 0.8625293917017369, 'exploration_final_eps': 0.14501659287007396, 'exploration_fraction': 0.24830713701557514, 'train_freq': 1, 'gamma': 0.9413413734415753, 'target_update_interval': 1236}. Best is trial 1 with value: 291.0.\n",
      "[I 2024-06-25 08:21:07,423] Trial 2 finished with value: 430.0 and parameters: {'learning_rate': 3.0451190769222607e-05, 'buffer_size': 3950, 'learning_starts': 1408, 'batch_size': 128, 'exploration_initial_eps': 0.8161688064600675, 'exploration_final_eps': 0.12165670503247585, 'exploration_fraction': 0.4553986214116369, 'train_freq': 8, 'gamma': 0.9868109595533288, 'target_update_interval': 4126}. Best is trial 2 with value: 430.0.\n",
      "[I 2024-06-25 08:21:21,824] Trial 3 finished with value: 576.0 and parameters: {'learning_rate': 0.0001860983885586858, 'buffer_size': 8688, 'learning_starts': 3742, 'batch_size': 128, 'exploration_initial_eps': 0.8137950872738898, 'exploration_final_eps': 0.15758086986186048, 'exploration_fraction': 0.19833722718618663, 'train_freq': 8, 'gamma': 0.9172403909149189, 'target_update_interval': 1653}. Best is trial 3 with value: 576.0.\n",
      "[I 2024-06-25 08:22:03,188] Trial 4 finished with value: 476.0 and parameters: {'learning_rate': 4.691030444418958e-05, 'buffer_size': 5052, 'learning_starts': 2342, 'batch_size': 32, 'exploration_initial_eps': 0.9998639674655108, 'exploration_final_eps': 0.11802917362061847, 'exploration_fraction': 0.14527905660308138, 'train_freq': 1, 'gamma': 0.9794981693006144, 'target_update_interval': 3277}. Best is trial 3 with value: 576.0.\n",
      "[I 2024-06-25 08:22:19,303] Trial 5 finished with value: 576.0 and parameters: {'learning_rate': 0.0001287230918834358, 'buffer_size': 3718, 'learning_starts': 2405, 'batch_size': 32, 'exploration_initial_eps': 0.9210439876506107, 'exploration_final_eps': 0.08104840821937997, 'exploration_fraction': 0.3347915852049388, 'train_freq': 4, 'gamma': 0.9870301829134348, 'target_update_interval': 3753}. Best is trial 3 with value: 576.0.\n",
      "[I 2024-06-25 08:22:40,914] Trial 6 finished with value: 552.0 and parameters: {'learning_rate': 0.00036845742626365, 'buffer_size': 3203, 'learning_starts': 1443, 'batch_size': 128, 'exploration_initial_eps': 0.9630635619890168, 'exploration_final_eps': 0.022541984030036006, 'exploration_fraction': 0.38458768139996946, 'train_freq': 4, 'gamma': 0.9796811615212603, 'target_update_interval': 1505}. Best is trial 3 with value: 576.0.\n",
      "[I 2024-06-25 08:23:19,003] Trial 7 finished with value: 474.0 and parameters: {'learning_rate': 0.00011063846241662296, 'buffer_size': 8651, 'learning_starts': 3711, 'batch_size': 64, 'exploration_initial_eps': 0.9504778639601578, 'exploration_final_eps': 0.17734154330549992, 'exploration_fraction': 0.1929388974553805, 'train_freq': 1, 'gamma': 0.932928473502553, 'target_update_interval': 2805}. Best is trial 3 with value: 576.0.\n",
      "[I 2024-06-25 08:23:29,063] Trial 8 finished with value: 332.0 and parameters: {'learning_rate': 6.913767944399614e-05, 'buffer_size': 4729, 'learning_starts': 3591, 'batch_size': 32, 'exploration_initial_eps': 0.9839283506790468, 'exploration_final_eps': 0.055044683669387764, 'exploration_fraction': 0.4640341882597222, 'train_freq': 8, 'gamma': 0.9232573365424659, 'target_update_interval': 1892}. Best is trial 3 with value: 576.0.\n",
      "[I 2024-06-25 08:23:40,863] Trial 9 finished with value: 576.0 and parameters: {'learning_rate': 0.00021655793789867486, 'buffer_size': 6442, 'learning_starts': 4798, 'batch_size': 32, 'exploration_initial_eps': 0.9923749178700513, 'exploration_final_eps': 0.18046641452426207, 'exploration_fraction': 0.22910132020428853, 'train_freq': 4, 'gamma': 0.913095997209302, 'target_update_interval': 1364}. Best is trial 3 with value: 576.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for DQN: {'learning_rate': 0.0001860983885586858, 'buffer_size': 8688, 'learning_starts': 3742, 'batch_size': 128, 'exploration_initial_eps': 0.8137950872738898, 'exploration_final_eps': 0.15758086986186048, 'exploration_fraction': 0.19833722718618663, 'train_freq': 8, 'gamma': 0.9172403909149189, 'target_update_interval': 1653}\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function\n",
    "def dqn_objective(trial):\n",
    "    policy_kwargs = dict(\n",
    "        activation_fn=torch.nn.ReLU,\n",
    "        net_arch=[128, 128]\n",
    "    )\n",
    "    \n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log = True)\n",
    "    buffer_size = trial.suggest_int('buffer_size', int(TIME_STEPS/10), TIME_STEPS)\n",
    "    learning_starts = trial.suggest_int('learning_starts', int(TIME_STEPS/10), int(TIME_STEPS/2))\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    exploration_initial_eps = trial.suggest_float('exploration_initial_eps', 0.8, 1.0)\n",
    "    exploration_final_eps = trial.suggest_float('exploration_final_eps', 0.01, 0.2)\n",
    "    exploration_fraction = trial.suggest_float('exploration_fraction', 0.1, 0.5)\n",
    "    train_freq = trial.suggest_categorical('train_freq', [1, 4, 8])\n",
    "    gamma = trial.suggest_float('gamma', 0.9, 0.9999)\n",
    "    target_update_interval = trial.suggest_int('target_update_interval', int(TIME_STEPS/10), int(TIME_STEPS/2))\n",
    "    \n",
    "    # Create the DQN model\n",
    "    model = DQN(\n",
    "        \"MlpPolicy\", \n",
    "        env, \n",
    "        policy_kwargs=policy_kwargs, \n",
    "        verbose=0,\n",
    "        learning_rate=learning_rate, \n",
    "        buffer_size=buffer_size, \n",
    "        learning_starts=learning_starts, \n",
    "        batch_size=batch_size, \n",
    "        exploration_initial_eps=exploration_initial_eps, \n",
    "        exploration_final_eps=exploration_final_eps, \n",
    "        exploration_fraction=exploration_fraction, \n",
    "        train_freq=train_freq, \n",
    "        gamma=gamma, \n",
    "        target_update_interval=target_update_interval\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.learn(total_timesteps=TIME_STEPS)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    mean_reward, _ = evaluate_policy(model, eval_env, n_eval_episodes=EVAL_EPISODES)\n",
    "    \n",
    "    return mean_reward\n",
    "\n",
    "# Optimize the hyperparameters\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(dqn_objective, n_trials=N_TRIALS)\n",
    "\n",
    "print('Best hyperparameters for DQN:', study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "\n",
    "dqn_model_best = DQN(\n",
    "    \"MlpPolicy\", \n",
    "    env, \n",
    "    policy_kwargs=policy_kwargs, \n",
    "    verbose=VERBOSE,\n",
    "    learning_rate=best_params['learning_rate'], \n",
    "    buffer_size=best_params['buffer_size'], \n",
    "    learning_starts=best_params['learning_starts'], \n",
    "    batch_size=best_params['batch_size'], \n",
    "    exploration_initial_eps=best_params['exploration_initial_eps'], \n",
    "    exploration_final_eps=best_params['exploration_final_eps'], \n",
    "    exploration_fraction=best_params['exploration_fraction'], \n",
    "    train_freq=best_params['train_freq'], \n",
    "    gamma=best_params['gamma'], \n",
    "    target_update_interval=best_params['target_update_interval'],\n",
    "    tensorboard_log=log_dir + 'dqn/'\n",
    ")\n",
    "dqn_model_best.learn(total_timesteps=TIME_STEPS)\n",
    "\n",
    "# Save the model\n",
    "dqn_model_best.save(\"models/dqn_tuned_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-25 08:23:53,553] A new study created in memory with name: no-name-a3f16185-ce1c-4457-85fe-0b5c7e15b298\n",
      "[I 2024-06-25 08:24:30,542] Trial 0 finished with value: 504.0 and parameters: {'learning_rate': 9.100415795715633e-05, 'n_steps': 18, 'gamma': 0.9156981864421981, 'gae_lambda': 0.9757035299314548, 'ent_coef': 0.0023917688408460463, 'vf_coef': 0.15642389309528187, 'max_grad_norm': 0.6656479073488772}. Best is trial 0 with value: 504.0.\n",
      "[I 2024-06-25 08:25:06,516] Trial 1 finished with value: 595.0 and parameters: {'learning_rate': 2.506651104514621e-05, 'n_steps': 19, 'gamma': 0.9265382649388079, 'gae_lambda': 0.9064011139751793, 'ent_coef': 0.003302037397252585, 'vf_coef': 0.9328739656455769, 'max_grad_norm': 0.5682185492162583}. Best is trial 1 with value: 595.0.\n",
      "[I 2024-06-25 08:25:48,255] Trial 2 finished with value: 240.0 and parameters: {'learning_rate': 0.00023639224585711639, 'n_steps': 18, 'gamma': 0.9196701466941525, 'gae_lambda': 0.8040393806170152, 'ent_coef': 3.182094258587533e-05, 'vf_coef': 0.35760184273609597, 'max_grad_norm': 0.8232802265214447}. Best is trial 1 with value: 595.0.\n",
      "[I 2024-06-25 08:26:28,572] Trial 3 finished with value: 552.0 and parameters: {'learning_rate': 3.973135978495046e-05, 'n_steps': 18, 'gamma': 0.9390583142169269, 'gae_lambda': 0.9514662929385624, 'ent_coef': 0.004228992670133673, 'vf_coef': 0.3557298335728514, 'max_grad_norm': 0.6348134156523917}. Best is trial 1 with value: 595.0.\n",
      "[I 2024-06-25 08:27:05,814] Trial 4 finished with value: 228.0 and parameters: {'learning_rate': 0.0005820887331923091, 'n_steps': 11, 'gamma': 0.9323634234780613, 'gae_lambda': 0.8056026568261961, 'ent_coef': 0.08650473981171838, 'vf_coef': 0.930817171141274, 'max_grad_norm': 0.7356428225251072}. Best is trial 1 with value: 595.0.\n",
      "[I 2024-06-25 08:27:40,965] Trial 5 finished with value: 430.0 and parameters: {'learning_rate': 0.00024015642917567674, 'n_steps': 17, 'gamma': 0.9335706534645692, 'gae_lambda': 0.9508565363967847, 'ent_coef': 0.0003339446779034021, 'vf_coef': 0.4049411624306536, 'max_grad_norm': 0.5751382599177943}. Best is trial 1 with value: 595.0.\n",
      "[I 2024-06-25 08:28:17,542] Trial 6 finished with value: 576.0 and parameters: {'learning_rate': 6.834826543339228e-05, 'n_steps': 19, 'gamma': 0.9814175802212366, 'gae_lambda': 0.8862116530297152, 'ent_coef': 0.07917540727798149, 'vf_coef': 0.8492548820104495, 'max_grad_norm': 0.6401977505014778}. Best is trial 1 with value: 595.0.\n",
      "[I 2024-06-25 08:28:57,570] Trial 7 finished with value: 477.0 and parameters: {'learning_rate': 0.0009100092354418166, 'n_steps': 11, 'gamma': 0.9803192100383019, 'gae_lambda': 0.876452665252156, 'ent_coef': 0.02297948158347005, 'vf_coef': 0.5737351025992949, 'max_grad_norm': 0.8403862619291764}. Best is trial 1 with value: 595.0.\n",
      "[I 2024-06-25 08:29:39,461] Trial 8 finished with value: 340.0 and parameters: {'learning_rate': 0.00010004265585698404, 'n_steps': 9, 'gamma': 0.9990735815949344, 'gae_lambda': 0.8966389787214288, 'ent_coef': 0.00046332647494608844, 'vf_coef': 0.2902195835255981, 'max_grad_norm': 0.5468408797741423}. Best is trial 1 with value: 595.0.\n",
      "[I 2024-06-25 08:30:19,811] Trial 9 finished with value: 370.0 and parameters: {'learning_rate': 2.32180570887014e-05, 'n_steps': 10, 'gamma': 0.9492691837766094, 'gae_lambda': 0.9094682945565203, 'ent_coef': 0.00016471460034288507, 'vf_coef': 0.5897629062090587, 'max_grad_norm': 0.8607991406019166}. Best is trial 1 with value: 595.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for A2C: {'learning_rate': 2.506651104514621e-05, 'n_steps': 19, 'gamma': 0.9265382649388079, 'gae_lambda': 0.9064011139751793, 'ent_coef': 0.003302037397252585, 'vf_coef': 0.9328739656455769, 'max_grad_norm': 0.5682185492162583}\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function\n",
    "def a2c_objective(trial):\n",
    "    policy_kwargs = dict(\n",
    "        activation_fn=torch.nn.ReLU,\n",
    "        net_arch=[128, 128]\n",
    "    )\n",
    "    \n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log = True)\n",
    "    n_steps = trial.suggest_int('n_steps', 5, 20)\n",
    "    gamma = trial.suggest_float('gamma', 0.9, 0.9999)\n",
    "    gae_lambda = trial.suggest_float('gae_lambda', 0.8, 1.0)\n",
    "    ent_coef = trial.suggest_float('ent_coef', 0.00001, 0.1, log = True)\n",
    "    vf_coef = trial.suggest_float('vf_coef', 0.1, 1.0)\n",
    "    max_grad_norm = trial.suggest_float('max_grad_norm', 0.3, 1.0)\n",
    "    \n",
    "    # Create the A2C model\n",
    "    model = A2C(\n",
    "        \"MlpPolicy\", \n",
    "        env, \n",
    "        policy_kwargs=policy_kwargs, \n",
    "        verbose=0,\n",
    "        learning_rate=learning_rate, \n",
    "        n_steps=n_steps, \n",
    "        gamma=gamma, \n",
    "        gae_lambda=gae_lambda, \n",
    "        ent_coef=ent_coef, \n",
    "        vf_coef=vf_coef, \n",
    "        max_grad_norm=max_grad_norm\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.learn(total_timesteps=TIME_STEPS)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    mean_reward, _ = evaluate_policy(model, eval_env, n_eval_episodes=EVAL_EPISODES)\n",
    "    \n",
    "    return mean_reward\n",
    "\n",
    "# Optimize the hyperparameters\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(a2c_objective, n_trials=N_TRIALS)\n",
    "\n",
    "print('Best hyperparameters for A2C:', study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "\n",
    "a2c_model_best = A2C(\n",
    "    \"MlpPolicy\", \n",
    "    env, \n",
    "    policy_kwargs=policy_kwargs, \n",
    "    verbose=VERBOSE,\n",
    "    learning_rate=best_params['learning_rate'], \n",
    "    n_steps=best_params['n_steps'], \n",
    "    gamma=best_params['gamma'], \n",
    "    gae_lambda=best_params['gae_lambda'], \n",
    "    ent_coef=best_params['ent_coef'], \n",
    "    vf_coef=best_params['vf_coef'], \n",
    "    max_grad_norm=best_params['max_grad_norm'],\n",
    "    tensorboard_log=log_dir + 'a2c/'\n",
    ")\n",
    "\n",
    "a2c_model_best.learn(total_timesteps=TIME_STEPS)\n",
    "\n",
    "# Save the model\n",
    "a2c_model_best.save(\"models/a2c_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the tuned RL models\n",
    "mean_reward_ppo, std_reward_ppo = evaluate_policy(ppo_model_best, eval_env, n_eval_episodes=EVAL_EPISODES, deterministic=True)\n",
    "mean_reward_dqn, std_reward_dqn = evaluate_policy(dqn_model_best, eval_env, n_eval_episodes=EVAL_EPISODES, deterministic=True)\n",
    "mean_reward_a2c, std_reward_a2c = evaluate_policy(a2c_model_best, eval_env, n_eval_episodes=EVAL_EPISODES, deterministic=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Agents and Compare Results\n",
    "Evaluate the performance of the agents and compare the best results obtained using the different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is A2C with a mean reward of 720.0\n",
      "Mean reward: 474.0\n",
      "Mean reward: 576.0\n",
      "Mean reward: 720.0\n"
     ]
    }
   ],
   "source": [
    "# Define the agents and their corresponding mean rewards and standard deviations\n",
    "agents = ['PPO', 'DQN', 'A2C']\n",
    "mean_rewards = [mean_reward_ppo, mean_reward_dqn, mean_reward_a2c]\n",
    "std_rewards = [std_reward_ppo, std_reward_dqn, std_reward_a2c]\n",
    "\n",
    "# Plotting the mean rewards\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(agents, mean_rewards, yerr=std_rewards, align='center', alpha=0.7, ecolor='black', capsize=10)\n",
    "plt.ylabel('Mean Reward')\n",
    "plt.title('Comparison of Mean Rewards of Different Agents')\n",
    "plt.show()\n",
    "\n",
    "# Determine the index of the best model\n",
    "best_index = np.argmax(mean_rewards)\n",
    "\n",
    "print(f\"The best model is {agents[best_index]} with a mean reward of {mean_rewards[best_index]}\")\n",
    "\n",
    "for r in mean_rewards:\n",
    "    print(f\"Mean reward: {r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2\n",
    "\n",
    "---\n",
    "\n",
    "## Enabling action masking, train and test a MaskablePPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20191678\\AppData\\Local\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.action_masks to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.action_masks` for environment variables or `env.get_wrapper_attr('action_masks')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 2219.0 +/- 0.0\n"
     ]
    }
   ],
   "source": [
    "# Define a function for the mask\n",
    "def mask_fn(env):\n",
    "    return env.get_mask()\n",
    "\n",
    "# Create an instance of the environment with mask enabled\n",
    "env = BoundedKnapsackEnv(n_items=200, max_weight=200, mask=True)\n",
    "\n",
    "# Wrap the environment with the ActionMasker\n",
    "vec_env = ActionMasker(env, mask_fn)\n",
    "\n",
    "# Create an evaluation environment\n",
    "eval_vec_env = Monitor(vec_env)\n",
    "\n",
    "# Define the policy architecture\n",
    "policy_kwargs = dict(\n",
    "    net_arch=dict(pi=[128, 128, 128], vf=[128, 128, 128]),  \n",
    ")\n",
    "\n",
    "# Train a MaskablePPO agent\n",
    "model = MaskablePPO(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=VERBOSE,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    learning_rate=3e-4,\n",
    ")\n",
    "\n",
    "# Adjust timesteps for meaningful training\n",
    "model.learn(total_timesteps=TIME_STEPS, use_masking=True)\n",
    "\n",
    "# Evaluate the agent\n",
    "mean_reward, std_reward = evaluate_policy_maskable(model, eval_vec_env, n_eval_episodes=EVAL_EPISODES, use_masking=True, deterministic=True)\n",
    "\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with different neural network architectures and tune the algorithm hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-25 08:50:33,258] A new study created in memory with name: no-name-728ca166-e19a-4f93-8432-c8156f1ff9aa\n",
      "[I 2024-06-25 08:52:01,835] Trial 0 finished with value: 2292.0 and parameters: {'learning_rate': 5.90997669988163e-05, 'n_steps': 3945, 'batch_size': 64, 'n_epochs': 7, 'gamma': 0.9656138871276837, 'gae_lambda': 0.8499390277718211, 'clip_range': 0.17122981067906945, 'ent_coef': 4.854392111683011e-05, 'max_grad_norm': 0.5595498373133915}. Best is trial 0 with value: 2292.0.\n",
      "[I 2024-06-25 08:53:41,375] Trial 1 finished with value: 2463.0 and parameters: {'learning_rate': 3.339302833522891e-05, 'n_steps': 2150, 'batch_size': 32, 'n_epochs': 8, 'gamma': 0.937693202613854, 'gae_lambda': 0.9299868394028932, 'clip_range': 0.1457262954413872, 'ent_coef': 6.02109474393273e-05, 'max_grad_norm': 0.9037298621736196}. Best is trial 1 with value: 2463.0.\n",
      "[I 2024-06-25 08:54:53,766] Trial 2 finished with value: 2616.0 and parameters: {'learning_rate': 1.1333726219664428e-05, 'n_steps': 2795, 'batch_size': 64, 'n_epochs': 5, 'gamma': 0.946690464852781, 'gae_lambda': 0.8899491763615047, 'clip_range': 0.10048982273168608, 'ent_coef': 1.6601297877804446e-05, 'max_grad_norm': 0.8073434993247872}. Best is trial 2 with value: 2616.0.\n",
      "[I 2024-06-25 08:56:47,627] Trial 3 finished with value: 1856.0 and parameters: {'learning_rate': 9.089875242416216e-05, 'n_steps': 4096, 'batch_size': 32, 'n_epochs': 10, 'gamma': 0.9139552398577244, 'gae_lambda': 0.998121477646132, 'clip_range': 0.23825600497998237, 'ent_coef': 0.00021311037195897095, 'max_grad_norm': 0.9069966015832449}. Best is trial 2 with value: 2616.0.\n",
      "[I 2024-06-25 08:57:51,120] Trial 4 finished with value: 2639.0 and parameters: {'learning_rate': 5.199784939759319e-05, 'n_steps': 3354, 'batch_size': 128, 'n_epochs': 8, 'gamma': 0.9675117568409538, 'gae_lambda': 0.8572440829789219, 'clip_range': 0.17248165234739712, 'ent_coef': 0.00031737766456303246, 'max_grad_norm': 0.7520324320363487}. Best is trial 4 with value: 2639.0.\n",
      "[I 2024-06-25 08:59:15,838] Trial 5 finished with value: 2331.0 and parameters: {'learning_rate': 7.38172445721355e-05, 'n_steps': 3804, 'batch_size': 32, 'n_epochs': 3, 'gamma': 0.9961442594599836, 'gae_lambda': 0.8012418424475825, 'clip_range': 0.1938445249566867, 'ent_coef': 0.00020322685648535368, 'max_grad_norm': 0.9821424392924342}. Best is trial 4 with value: 2639.0.\n",
      "[I 2024-06-25 09:01:32,638] Trial 6 finished with value: 2623.0 and parameters: {'learning_rate': 1.6990090015688637e-05, 'n_steps': 3086, 'batch_size': 32, 'n_epochs': 7, 'gamma': 0.9865661545859304, 'gae_lambda': 0.9256501646971605, 'clip_range': 0.25212052981801825, 'ent_coef': 0.0012584442511783807, 'max_grad_norm': 0.9577301344696624}. Best is trial 4 with value: 2639.0.\n",
      "[I 2024-06-25 09:03:42,998] Trial 7 finished with value: 2383.0 and parameters: {'learning_rate': 4.976679965335868e-05, 'n_steps': 3675, 'batch_size': 32, 'n_epochs': 7, 'gamma': 0.9419249353468961, 'gae_lambda': 0.9897095731355798, 'clip_range': 0.20040771551540976, 'ent_coef': 0.001157251835251321, 'max_grad_norm': 0.8517723151486016}. Best is trial 4 with value: 2639.0.\n",
      "[I 2024-06-25 09:05:23,360] Trial 8 finished with value: 2315.0 and parameters: {'learning_rate': 8.541621953697806e-05, 'n_steps': 2384, 'batch_size': 64, 'n_epochs': 5, 'gamma': 0.9136108869427159, 'gae_lambda': 0.9863156762650884, 'clip_range': 0.21197214980289708, 'ent_coef': 0.0006174843219709019, 'max_grad_norm': 0.8952137810965484}. Best is trial 4 with value: 2639.0.\n",
      "[I 2024-06-25 09:07:06,092] Trial 9 finished with value: 2538.0 and parameters: {'learning_rate': 2.4483033602110006e-05, 'n_steps': 3731, 'batch_size': 128, 'n_epochs': 9, 'gamma': 0.9906930781260875, 'gae_lambda': 0.8245722038499996, 'clip_range': 0.15018874636560756, 'ent_coef': 0.007793617040236528, 'max_grad_norm': 0.7891338685181871}. Best is trial 4 with value: 2639.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for MaskablePPO: {'learning_rate': 5.199784939759319e-05, 'n_steps': 3354, 'batch_size': 128, 'n_epochs': 8, 'gamma': 0.9675117568409538, 'gae_lambda': 0.8572440829789219, 'clip_range': 0.17248165234739712, 'ent_coef': 0.00031737766456303246, 'max_grad_norm': 0.7520324320363487}\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function for MaskablePPO\n",
    "def maskableppo_objective(trial):\n",
    "    policy_kwargs = dict(\n",
    "        net_arch=dict(pi=[128, 128, 128], vf=[128, 128, 128]),  \n",
    "    )\n",
    "\n",
    "    # Adjusted hyperparameters suggestion\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-4, log=True)  # Narrower range\n",
    "    n_steps = trial.suggest_int('n_steps', 2048, 4096)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    n_epochs = trial.suggest_int('n_epochs', 3, 10)\n",
    "    gamma = trial.suggest_float('gamma', 0.9, 0.9999)\n",
    "    gae_lambda = trial.suggest_float('gae_lambda', 0.8, 1.0)\n",
    "    clip_range = trial.suggest_float('clip_range', 0.1, 0.3)  # Adjusted range\n",
    "    ent_coef = trial.suggest_float('ent_coef', 0.00001, 0.01, log=True)  # Narrower range\n",
    "    max_grad_norm = trial.suggest_float('max_grad_norm', 0.5, 1.0)  # Gradient clipping\n",
    "\n",
    "    model = MaskablePPO(\n",
    "        \"MlpPolicy\", \n",
    "        vec_env, \n",
    "        policy_kwargs=policy_kwargs, \n",
    "        verbose=0,\n",
    "        learning_rate=learning_rate, \n",
    "        n_steps=n_steps, \n",
    "        batch_size=batch_size, \n",
    "        n_epochs=n_epochs, \n",
    "        gamma=gamma, \n",
    "        gae_lambda=gae_lambda, \n",
    "        clip_range=clip_range, \n",
    "        ent_coef=ent_coef,\n",
    "        max_grad_norm=max_grad_norm\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=TIME_STEPS, use_masking=True)\n",
    "\n",
    "    mean_reward, _ = evaluate_policy_maskable(model, eval_vec_env, n_eval_episodes=EVAL_EPISODES, use_masking=True, deterministic=True)\n",
    "\n",
    "    return mean_reward\n",
    "\n",
    "# Optimize the hyperparameters\n",
    "maskableppo_study = optuna.create_study(direction='maximize')\n",
    "maskableppo_study.optimize(maskableppo_objective, n_trials=N_TRIALS)\n",
    "\n",
    "print('Best hyperparameters for MaskablePPO:', maskableppo_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward for the best MaskablePPO agent: 2633.0 +/- 0.0\n"
     ]
    }
   ],
   "source": [
    "# Train the best MaskablePPO model\n",
    "best_params = maskableppo_study.best_params\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    net_arch=dict(pi=[128, 128, 128], vf=[128, 128, 128]),  \n",
    ")\n",
    "\n",
    "maskableppo_model_best = MaskablePPO(\n",
    "    \"MlpPolicy\", \n",
    "    vec_env, \n",
    "    policy_kwargs=policy_kwargs, \n",
    "    verbose=VERBOSE,\n",
    "    learning_rate=best_params['learning_rate'], \n",
    "    n_steps=best_params['n_steps'], \n",
    "    batch_size=best_params['batch_size'], \n",
    "    n_epochs=best_params['n_epochs'], \n",
    "    gamma=best_params['gamma'], \n",
    "    gae_lambda=best_params['gae_lambda'], \n",
    "    clip_range=best_params['clip_range'], \n",
    "    ent_coef=best_params['ent_coef'],\n",
    "    tensorboard_log=log_dir + 'maskableppo'\n",
    ")\n",
    "maskableppo_model_best.learn(total_timesteps=TIME_STEPS, use_masking=True)\n",
    "\n",
    "# Save the best model\n",
    "maskableppo_model_best.save(\"models/maskableppo_tuned_model\")\n",
    "\n",
    "# Evaluate the best model\n",
    "mean_reward, std_reward = evaluate_policy_maskable(maskableppo_model_best, eval_vec_env, n_eval_episodes=EVAL_EPISODES, use_masking=True, deterministic=True)\n",
    "print(f\"Mean reward for the best MaskablePPO agent: {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Evaluate the agent then compare the best results obtained with those of the best agent from Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 - Mean reward: 720.0 +/- 0.0\n",
      "Part 2 - Mean reward: 2633.0 +/- 0.0\n"
     ]
    }
   ],
   "source": [
    "# Load the best model from Part 1\n",
    "if agents[best_index] == 'PPO':\n",
    "    best_model_part1 = PPO.load(\"models/ppo_tuned_model\", env=env)\n",
    "elif agents[best_index] == 'DQN':\n",
    "    best_model_part1 = DQN.load(\"models/dqn_tuned_model\", env=env)\n",
    "elif agents[best_index] == 'A2C':\n",
    "    best_model_part1 = A2C.load(\"models/a2c_tuned_model\", env=env)\n",
    "else:\n",
    "    print(\"Unknown model type\")\n",
    "\n",
    "# Load the best model from Part 2\n",
    "best_model_part2 = MaskablePPO.load(\"models/maskableppo_tuned_model\", env=vec_env)\n",
    "\n",
    "# Evaluate the best model from Part 1\n",
    "mean_reward_part1, std_reward_part1 = evaluate_policy(best_model_part1, eval_env, n_eval_episodes=EVAL_EPISODES, deterministic=True)\n",
    "\n",
    "# Evaluate the best model from Part 2\n",
    "mean_reward_part2, std_reward_part2 = evaluate_policy_maskable(best_model_part2, eval_vec_env, n_eval_episodes=EVAL_EPISODES, deterministic=True, use_masking=True)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Part 1 - Mean reward: {mean_reward_part1} +/- {std_reward_part1}\")\n",
    "print(f\"Part 2 - Mean reward: {mean_reward_part2} +/- {std_reward_part2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
